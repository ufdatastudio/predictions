{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2690733",
   "metadata": {},
   "source": [
    "# Tutorial: Creating a Hybrid Retrieval Pipeline\n",
    "\n",
    "- **Level**: Intermediate\n",
    "- **Time to complete**: 15 minutes\n",
    "- **Components Used**: [`DocumentSplitter`](https://docs.haystack.deepset.ai/docs/documentsplitter), [`SentenceTransformersDocumentEmbedder`](https://docs.haystack.deepset.ai/docs/sentencetransformersdocumentembedder), [`DocumentJoiner`](https://docs.haystack.deepset.ai/docs/documentjoiner), [`InMemoryDocumentStore`](https://docs.haystack.deepset.ai/docs/inmemorydocumentstore), [`InMemoryBM25Retriever`](https://docs.haystack.deepset.ai/docs/inmemorybm25retriever), [`InMemoryEmbeddingRetriever`](https://docs.haystack.deepset.ai/docs/inmemoryembeddingretriever), and [`TransformersSimilarityRanker`](https://docs.haystack.deepset.ai/docs/transformerssimilarityranker)\n",
    "- **Prerequisites**: None\n",
    "- **Goal**: After completing this tutorial, you will have learned about creating a hybrid retrieval and when it's useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aeb4f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from datasets import load_dataset\n",
    "from haystack import Document\n",
    "\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(os.path.join(notebook_dir, '../'))\n",
    "\n",
    "# import log_files\n",
    "from data_processing import DataProcessing\n",
    "from text_generation_models import TextGenerationModelFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10be77b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83349efc",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "- **Problems:**\n",
    "    1. Multiple files with distinct retrieved results\n",
    "    2. Same file where we duplicate retrieved results. Duplicate as in first run: 0 to 7, second run 7 to 14, but it includes 0 to 7, so file would be 0 to 7, 0 to 7, then 7 to 14.\n",
    "\n",
    "- `new_file = True:` Only when it's the first usage.\n",
    "- `new_file = False:` Only when it's after the first usage. Reasoning is: the next step of LLM labeling (prediction/non-prediction) will use this saved file and store results in same file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "138b7b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53987c086684d0386903fb6af80f938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentiment', 'sentence'],\n",
       "    num_rows: 4846\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_file = True\n",
    "base_data_path = os.path.join(notebook_dir, '../data/', 'financial_phrase_bank')\n",
    "if new_file == True: \n",
    "    new_file_name = \"all_data-adjusted_header.csv\"\n",
    "    financial_full_path = os.path.join(base_data_path, new_file_name)\n",
    "    df = load_dataset(\"csv\", data_files=financial_full_path, split=\"train\")\n",
    "\n",
    "else:\n",
    "    new_file_name = \"text_label_name_meta_data-v1.csv\"\n",
    "    financial_full_path = os.path.join(base_data_path, new_file_name)\n",
    "    df = load_dataset(\"csv\", data_files=financial_full_path, split=\"train\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467a7fe",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Hybrid Retrieval** combines keyword-based and embedding-based retrieval techniques, leveraging the strengths of both approaches. In essence, dense embeddings excel in grasping the contextual nuances of the query, while keyword-based methods excel in matching keywords.\n",
    "\n",
    "There are many cases when a simple keyword-based approaches like BM25 performs better than a dense retrieval (for example in a specific domain like healthcare) because a dense model needs to be trained on data. For more details about Hybrid Retrieval, check out [Blog Post: Hybrid Document Retrieval](https://haystack.deepset.ai/blog/hybrid-retrieval)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802b1016",
   "metadata": {},
   "source": [
    "## Initializing the DocumentStore\n",
    "\n",
    "You'll start creating your question answering system by initializing a DocumentStore. A DocumentStore stores the Documents that your system uses to find answers to your questions. In this tutorial, you'll be using the [`InMemoryDocumentStore`](https://docs.haystack.deepset.ai/docs/inmemorydocumentstore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738600c1",
   "metadata": {},
   "source": [
    "> `InMemoryDocumentStore` is the simplest DocumentStore to get started with. It requires no external dependencies and it's a good option for smaller projects and debugging. But it doesn't scale up so well to larger Document collections, so it's not a good choice for production systems. To learn more about the different types of external databases that Haystack supports, see [DocumentStore Integrations](https://haystack.deepset.ai/integrations?type=Document+Store&version=2.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c9350",
   "metadata": {},
   "source": [
    "## Fetching and Processing Documents\n",
    "\n",
    "For searching, use the *sentence* feature (column). The other features (columns) will be stored as metadata for [metadata filtering](https://docs.haystack.deepset.ai/docs/metadata-filtering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for doc in dataset:\n",
    "    docs.append(\n",
    "        Document(content=doc[\"sentence\"], meta={\"sentiment\": doc[\"sentiment\"]})\n",
    "    )\n",
    "docs = docs[:7]\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea9968",
   "metadata": {},
   "source": [
    "## Indexing Documents with a Pipeline\n",
    "\n",
    "Create a pipeline to store the data in the document store with their embedding. For this pipeline, you need a [DocumentSplitter](https://docs.haystack.deepset.ai/docs/documentsplitter) to split documents into chunks of 512 words, [SentenceTransformersDocumentEmbedder](https://docs.haystack.deepset.ai/docs/sentencetransformersdocumentembedder) to create document embeddings for dense retrieval and [DocumentWriter](https://docs.haystack.deepset.ai/docs/documentwriter) to write documents to the document store.\n",
    "\n",
    "As an embedding model, you will use [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) on Hugging Face. Feel free to test other models on Hugging Face or use another [Embedder](https://docs.haystack.deepset.ai/docs/embedders) to switch the model provider.\n",
    "\n",
    "> If this step takes too long for you, replace the embedding model with a smaller model such as `sentence-transformers/all-MiniLM-L6-v2` or `sentence-transformers/all-mpnet-base-v2`. Make sure that the `split_length` is updated according to your model's token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.preprocessors.document_splitter import DocumentSplitter\n",
    "from haystack import Pipeline\n",
    "from haystack.utils import ComponentDevice\n",
    "\n",
    "document_splitter = DocumentSplitter(split_by=\"word\", split_length=512, split_overlap=32)\n",
    "document_embedder = SentenceTransformersDocumentEmbedder(\n",
    "    model=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "document_writer = DocumentWriter(document_store)\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"document_splitter\", document_splitter) # sentences/documents alone\n",
    "indexing_pipeline.add_component(\"document_embedder\", document_embedder) # embeddings per document\n",
    "indexing_pipeline.add_component(\"document_writer\", document_writer) # documents -> vector store\n",
    "\n",
    "indexing_pipeline.connect(\"document_splitter\", \"document_embedder\") # map documents : embeddings\n",
    "indexing_pipeline.connect(\"document_embedder\", \"document_writer\") # embeddings : vector store\n",
    "\n",
    "indexing_pipeline.run({\"document_splitter\": {\"documents\": docs}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8530902",
   "metadata": {},
   "source": [
    "Documents are stored in `InMemoryDocumentStore` with their embeddings, now it's time for creating the hybrid retrieval pipeline âœ…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a0a3f",
   "metadata": {},
   "source": [
    "## Creating a Pipeline for Hybrid Retrieval\n",
    "\n",
    "Hybrid retrieval refers to the combination of multiple retrieval methods to enhance overall performance. In the context of search systems, a hybrid retrieval pipeline executes both traditional keyword-based search and dense vector search, later ranking the results with a **cross-encoder model**. This combination allows the search system to leverage the strengths of different approaches, providing more accurate and diverse results.\n",
    "\n",
    "Here are the required steps for a hybrid retrieval pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7e6c50",
   "metadata": {},
   "source": [
    "### 1) Initialize Retrievers and the Embedder\n",
    "\n",
    "Initialize a [InMemoryEmbeddingRetriever](https://docs.haystack.deepset.ai/docs/inmemoryembeddingretriever) and [InMemoryBM25Retriever](https://docs.haystack.deepset.ai/docs/inmemorybm25retriever) to perform both dense and keyword-based retrieval. For dense retrieval, you also need a [SentenceTransformersTextEmbedder](https://docs.haystack.deepset.ai/docs/sentencetransformerstextembedder) that computes the embedding of the search query by using the same embedding model `BAAI/bge-small-en-v1.5` that was used in the indexing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever, InMemoryEmbeddingRetriever\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "\n",
    "text_embedder = SentenceTransformersTextEmbedder(\n",
    "    model=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "embedding_retriever = InMemoryEmbeddingRetriever(document_store)\n",
    "bm25_retriever = InMemoryBM25Retriever(document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1377db",
   "metadata": {},
   "source": [
    "### 2) Join Retrieval Results\n",
    "\n",
    "Haystack offers several joining methods in [`DocumentJoiner`](https://docs.haystack.deepset.ai/docs/documentjoiner) to be used for different use cases such as `merge` and `reciprocal_rank_fusion`. In this example, you will use the default `concatenate` mode to join the documents coming from two Retrievers as the [Ranker](https://docs.haystack.deepset.ai/docs/rankers) will be the main component to rank the documents for relevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.joiners import DocumentJoiner\n",
    "\n",
    "document_joiner = DocumentJoiner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77fbcc6",
   "metadata": {},
   "source": [
    "### 3) Rank the Results\n",
    "\n",
    "Use the [TransformersSimilarityRanker](https://docs.haystack.deepset.ai/docs/transformerssimilarityranker) that scores the relevancy of all retrieved documents for the given search query by using a cross encoder model. In this example, you will use [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) model to rank the retrieved documents but you can replace this model with other cross-encoder models on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b07c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.rankers import TransformersSimilarityRanker\n",
    "\n",
    "ranker = TransformersSimilarityRanker(model=\"BAAI/bge-reranker-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7002e2",
   "metadata": {},
   "source": [
    "### 4) Create the Hybrid Retrieval Pipeline\n",
    "\n",
    "Add all initialized components to your pipeline and connect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "\n",
    "hybrid_retrieval = Pipeline()\n",
    "hybrid_retrieval.add_component(\"text_embedder\", text_embedder)\n",
    "hybrid_retrieval.add_component(\"embedding_retriever\", embedding_retriever)\n",
    "hybrid_retrieval.add_component(\"bm25_retriever\", bm25_retriever)\n",
    "hybrid_retrieval.add_component(\"document_joiner\", document_joiner)\n",
    "hybrid_retrieval.add_component(\"ranker\", ranker)\n",
    "\n",
    "hybrid_retrieval.connect(\"text_embedder\", \"embedding_retriever\")\n",
    "hybrid_retrieval.connect(\"bm25_retriever\", \"document_joiner\")\n",
    "hybrid_retrieval.connect(\"embedding_retriever\", \"document_joiner\")\n",
    "hybrid_retrieval.connect(\"document_joiner\", \"ranker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f5f6ac",
   "metadata": {},
   "source": [
    "### 5) Visualize the Pipeline (Optional)\n",
    "\n",
    "To understand how you formed a hybrid retrieval pipeline, use [draw()](https://docs.haystack.deepset.ai/docs/drawing-pipeline-graphs) method of the pipeline. If you're running this notebook on Google Colab, the generate file will be saved in \"Files\" section on the sidebar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff346f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_img_name = \"hybrid-retrieval.png\"\n",
    "# save_image_path = os.path.join(notebook_dir, \"../data/hybrid_retrieval/\", save_img_name)\n",
    "# hybrid_retrieval.draw(path=save_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd7e4d",
   "metadata": {},
   "source": [
    "## Testing the Hybrid Retrieval\n",
    "\n",
    "Pass the query to `text_embedder`, `bm25_retriever` and `ranker` and run the retrieval pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48078227",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_properties = DataProcessing.load_prediction_properties()\n",
    "prediction_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da592f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_verbs = [\n",
    "    \"will\",\n",
    "    \"shall\",\n",
    "    \"would\",\n",
    "    \"going\",\n",
    "    \"might\",\n",
    "    \"should\",\n",
    "    \"could\",\n",
    "    \"may\",\n",
    "    \"must\",\n",
    "    \"can\"\n",
    "]\n",
    "query = f\"\"\"Can you identify the predictions from the documents? I define a prediction as: {prediction_properties}. Note that the documents should be future tense like {future_verbs}.\n",
    "    \n",
    "    Some examples of predictions in the PhraseBank dataset are \\n\n",
    "        1. According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing . \\n\n",
    "        2. According to the company 's updated strategy for the years 2009-2012 , Basware targets a long-term net sales growth in the range of 20 % -40 % with an operating profit margin of 10 % -20 % of net sales .\n",
    "        3. Its board of directors will propose a dividend of EUR0 .12 per share for 2010 , up from the EUR0 .08 per share paid in 2009 .\n",
    "    Some examples of non-predictions in the P\n",
    "        1. Net sales increased to EUR193 .3 m from EUR179 .9 m and pretax profit rose by 34.2 % to EUR43 .1 m. ( EUR1 = USD1 .4 )\n",
    "        2. Net sales surged by 18.5 % to EUR167 .8 m. Teleste said that EUR20 .4 m , or 12.2 % , of the sales came from the acquisitions made in 2009 .\n",
    "        3. STORA ENSO , NORSKE SKOG , M-REAL , UPM-KYMMENE Credit Suisse First Boston ( CFSB ) raised the fair value for shares in four of the largest Nordic forestry groups .\n",
    "\"\"\"\n",
    "\n",
    "retrieved_result = hybrid_retrieval.run(\n",
    "    {\"text_embedder\": {\"text\": query}, \"bm25_retriever\": {\"query\": query}, \"ranker\": {\"query\": query}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3051b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_result[\"ranker\"][\"documents\"]\n",
    "retrieved_result_df = pd.DataFrame(retrieved_result[\"ranker\"][\"documents\"])\n",
    "retrieved_result_df['query'] = query\n",
    "retrieved_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac31c8c",
   "metadata": {},
   "source": [
    "## Save Data\n",
    "\n",
    "- **Problems:**\n",
    "    1. Multiple files with distinct retrieved results\n",
    "    2. Same file where we duplicate retrieved results. Duplicate as in first run: 0 to 7, second run 7 to 14, but it includes 0 to 7, so file would be 0 to 7, 0 to 7, then 7 to 14.\n",
    "\n",
    "- `new_file = True:` Only when it's the first usage.\n",
    "- `new_file = False:` Only when it's after the first usage. Reasoning is: the next step of LLM labeling (prediction/non-prediction) will use this saved file and store results in same file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_file = True:\n",
    "    path = os.path.join(notebook_dir, \"../data/rag/retrieved/\")\n",
    "    prefix = \"retrieved_results\"\n",
    "    save_file_type = \"csv\"\n",
    "    DataProcessing.save_to_file(retrieved_result_df, path, prefix, save_file_type)\n",
    "else:\n",
    "    \"\"\"If append to previous file, then save:\"\"\"\n",
    "    path = os.path.join(notebook_dir, \"../data/rag/retrieved/\", \"text_label_name_meta_data-v1.csv\")\n",
    "    # prefix = \"text_label_metadata\"\n",
    "    save_file_type = \"csv\"\n",
    "    df = DataProcessing.load_from_file(path, save_file_type)\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89909f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = retrieved_result_df.content\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a012b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_df = retrieved_result_df.drop(['content'], axis=1)\n",
    "meta_data_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(content)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1274829",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_datas = []\n",
    "for idx, row in meta_data_df.iterrows():\n",
    "    meta_datas.append(row)\n",
    "\n",
    "meta_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f16bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['meta_data'] = meta_datas\n",
    "new_df.rename(columns={'content': 'text'}, inplace=True)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488f6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = pd.concat([df, new_df])\n",
    "updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05343d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(notebook_dir, \"../data/rag/retrieved/\")\n",
    "prefix = \"text_label_name_meta_data\"\n",
    "save_file_type = \"csv\"\n",
    "DataProcessing.save_to_file(updated_df, path, prefix, save_file_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534e3f5",
   "metadata": {},
   "source": [
    "### Pretty Print the Results\n",
    "Create a function to print a kind of *search page*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_results(prediction):\n",
    "    for doc in prediction[\"documents\"]:\n",
    "        print(doc.content, \"\\t\", doc.score)\n",
    "        # print(doc.meta[\"abstract\"])\n",
    "        print(\"\\n\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afbb05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_results(result[\"ranker\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f84f0",
   "metadata": {},
   "source": [
    "## What's next\n",
    "\n",
    "ðŸŽ‰ Congratulations! You've create a hybrid retrieval pipeline!\n",
    "\n",
    "If you'd like to use this retrieval method in a RAG pipeline, check out [Tutorial: Creating Your First QA Pipeline with Retrieval-Augmentation](https://haystack.deepset.ai/tutorials/27_first_rag_pipeline) to learn about the next steps.\n",
    "\n",
    "To stay up to date on the latest Haystack developments, you can [sign up for our newsletter](https://landing.deepset.ai/haystack-community-updates) or [join Haystack discord community](https://discord.gg/haystack).\n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa6f3de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11fe4a15",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
