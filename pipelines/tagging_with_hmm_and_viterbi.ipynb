{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling Prediction Sentences\n",
    "\n",
    "1. **Research Focus**\n",
    "    - **Main Idea:** Certifying textual predictions given some observations\n",
    "2. **Task**:\n",
    "    - **Main Idea:** Sequence Labeling is the action of tagging each term in the sequence\n",
    "3. **Methods:**\n",
    "    - **Main Idea:** Learn the structure of the sequence\n",
    "4. **Decoding Techniques:**\n",
    "    - **Main Idea:** Discuss Viterbi and Greedy\n",
    "6. **Feedback x Q&A x Misc:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Focus\n",
    "\n",
    "- **Main Idea:** Certifying textual predictions given some observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mathematical Representation of Prediction:**\n",
    "\n",
    "    $$\n",
    "    P = \\{p_1, p_2, ..., p_N\\}, \\text{where} \\\\\n",
    "    p_i = (p_{source}, p_{target}, p_{date}, p_{outcome})\n",
    "    $$\n",
    "\n",
    "- **Mathematical Representation of Observation:**\n",
    "\n",
    "    $$\n",
    "    O = \\{o_1, o_2, ..., o_M\\}, \\text{where} \\\\\n",
    "    o_i = (o_{source}, o_{target}, o_{date}, o_{outcome})\n",
    "    $$\n",
    "- **Difference between $ P $ and $ O $:**\n",
    "    1. $ P $ is future tense\n",
    "    2. $ O $ is past tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to load P and O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "- **Main Idea:** Sequence Labeling is the action of providing a certain label to each term in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Label Examples:**\n",
    "\n",
    "    1. Part-of-Speech (POS)\n",
    "    2. Named Entities \n",
    "    3. BOIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "- **Main Idea:** Learn the structure of the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Manually\n",
    "1. Rules\n",
    "    1. Think: if then statements\n",
    "    2. Curated by lingusts\n",
    "    3. Too many combinations\n",
    "    4. Tags can be ambiguous \n",
    "2.  Hidden Markov Model (HMM)\n",
    "    1. Markov Model\n",
    "        1. Developed by: Andrei A. Markov in 1913\n",
    "    2. Probabilistic based\n",
    "    2. Sequential model based on the Markov\n",
    "    2. Utilizes the joint distribution\n",
    "    2. Dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Update Data\n",
    "- [x] Find a way to separate sentences when loading the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str, file_name: str, is_test_file: bool, config_index: bool = True):\n",
    "    \n",
    "    if config_index == True:\n",
    "        if is_test_file != True:\n",
    "            file =  file_path + file_name\n",
    "            open_df = pd.read_table(file, sep = \"\\t\", names=['Index', 'Term', 'BIO x Prediction Tag'], skip_blank_lines=False)\n",
    "        else:\n",
    "            file =  file_path + file_name\n",
    "            open_df = pd.read_table(file, sep = \"\\t\", names=['Index', 'Word'], skip_blank_lines=False)\n",
    "        \n",
    "    return open_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df_rows_with_dummy(df: pd.DataFrame, new_columns_name: list) -> pd.DataFrame:  \n",
    "    \"\"\"Update the rows of the dataframe if blank space, fill with dummy\"\"\"  \n",
    "\n",
    "    dummy_row = pd.DataFrame([['0.0', ' ', 'dummy']], columns=df.columns)\n",
    "    df = pd.concat([dummy_row, df], ignore_index=True)\n",
    "    df.columns = new_columns_name\n",
    "    df.fillna(\"dummy\", inplace=True)\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data('../data/tagging/official/', 'train', False)\n",
    "dev_df = load_data('../data/tagging/official/', 'dev', False)\n",
    "test_df = load_data('../data/tagging/official/', 'test', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_columns_name = ['Index', 'Word', 'POS Tag']\n",
    "\n",
    "updated_train_df = update_df_rows_with_dummy(train_df, train_dev_columns_name)\n",
    "updated_dev_df = update_df_rows_with_dummy(dev_df, train_dev_columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2551</td>\n",
       "      <td>15</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index Word POS Tag\n",
       "48  2551   15   I-p_d"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dev_df.loc[48:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2504</td>\n",
       "      <td>Detravious</td>\n",
       "      <td>B-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2505</td>\n",
       "      <td>,</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2506</td>\n",
       "      <td>a</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2507</td>\n",
       "      <td>financial</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2508</td>\n",
       "      <td>analyst</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2509</td>\n",
       "      <td>forecasts</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2510</td>\n",
       "      <td>that</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2511</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2512</td>\n",
       "      <td>stock</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2513</td>\n",
       "      <td>price</td>\n",
       "      <td>I-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2514</td>\n",
       "      <td>at</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2515</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>B-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2516</td>\n",
       "      <td>will</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2517</td>\n",
       "      <td>likely</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2518</td>\n",
       "      <td>decrease</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2519</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2520</td>\n",
       "      <td>2027</td>\n",
       "      <td>B-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2521</td>\n",
       "      <td>Q3</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2522</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2523</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2524</td>\n",
       "      <td>On</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2525</td>\n",
       "      <td>August</td>\n",
       "      <td>B-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2526</td>\n",
       "      <td>21</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2527</td>\n",
       "      <td>,</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2528</td>\n",
       "      <td>2024</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2529</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2530</td>\n",
       "      <td>Goldman</td>\n",
       "      <td>B-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2531</td>\n",
       "      <td>Sachs</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2532</td>\n",
       "      <td>speculates</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2533</td>\n",
       "      <td>that</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2534</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2535</td>\n",
       "      <td>operating</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2536</td>\n",
       "      <td>cash</td>\n",
       "      <td>I-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2537</td>\n",
       "      <td>flow</td>\n",
       "      <td>I-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2538</td>\n",
       "      <td>at</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2539</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>B-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2540</td>\n",
       "      <td>will</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2541</td>\n",
       "      <td>likely</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2542</td>\n",
       "      <td>increase</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2543</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2544</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2545</td>\n",
       "      <td>Morgan</td>\n",
       "      <td>B-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2546</td>\n",
       "      <td>Stanley</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2547</td>\n",
       "      <td>predicts</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2548</td>\n",
       "      <td>that</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2549</td>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2550</td>\n",
       "      <td>September</td>\n",
       "      <td>B-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2551</td>\n",
       "      <td>15</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2552</td>\n",
       "      <td>,</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2553</td>\n",
       "      <td>2025</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2554</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2555</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2556</td>\n",
       "      <td>S&amp;P</td>\n",
       "      <td>B-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2557</td>\n",
       "      <td>500</td>\n",
       "      <td>I-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2558</td>\n",
       "      <td>index</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2559</td>\n",
       "      <td>will</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2560</td>\n",
       "      <td>likely</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2561</td>\n",
       "      <td>rise</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2562</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2563</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2564</td>\n",
       "      <td>According</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2565</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2566</td>\n",
       "      <td>Apple</td>\n",
       "      <td>B-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2567</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2568</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2569</td>\n",
       "      <td>projected</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2570</td>\n",
       "      <td>revenue</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2571</td>\n",
       "      <td>at</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2572</td>\n",
       "      <td>Alphabet</td>\n",
       "      <td>B-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2573</td>\n",
       "      <td>will</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2574</td>\n",
       "      <td>likely</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2575</td>\n",
       "      <td>fall</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2576</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2577</td>\n",
       "      <td>Q4</td>\n",
       "      <td>B-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2578</td>\n",
       "      <td>2026</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2579</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2580</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2581</td>\n",
       "      <td>In</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2582</td>\n",
       "      <td>Q2</td>\n",
       "      <td>B-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2583</td>\n",
       "      <td>2027</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2584</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2585</td>\n",
       "      <td>Wells</td>\n",
       "      <td>B-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2586</td>\n",
       "      <td>Fargo</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2587</td>\n",
       "      <td>expects</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2588</td>\n",
       "      <td>that</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2589</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2590</td>\n",
       "      <td>inflation</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2591</td>\n",
       "      <td>rate</td>\n",
       "      <td>I-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2592</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2593</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2594</td>\n",
       "      <td>US</td>\n",
       "      <td>B-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2595</td>\n",
       "      <td>will</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2596</td>\n",
       "      <td>likely</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2597</td>\n",
       "      <td>stay</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2598</td>\n",
       "      <td>stable</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2599</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2600</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2601</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2602</td>\n",
       "      <td>Dow</td>\n",
       "      <td>B-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2603</td>\n",
       "      <td>Jones</td>\n",
       "      <td>I-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2604</td>\n",
       "      <td>Industrial</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2605</td>\n",
       "      <td>Average</td>\n",
       "      <td>I-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2606</td>\n",
       "      <td>will</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2607</td>\n",
       "      <td>likely</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2608</td>\n",
       "      <td>rise</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2609</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2610</td>\n",
       "      <td>2029</td>\n",
       "      <td>B-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2611</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2612</td>\n",
       "      <td>according</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2613</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2614</td>\n",
       "      <td>JPMorgan</td>\n",
       "      <td>B-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2615</td>\n",
       "      <td>Chase</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2616</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2617</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2618</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2619</td>\n",
       "      <td>World</td>\n",
       "      <td>B-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2620</td>\n",
       "      <td>Health</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2621</td>\n",
       "      <td>Organization</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2622</td>\n",
       "      <td>forecasts</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2623</td>\n",
       "      <td>that</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2624</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2625</td>\n",
       "      <td>prevalence</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2626</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2627</td>\n",
       "      <td>chronic</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2628</td>\n",
       "      <td>illnesses</td>\n",
       "      <td>I-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2629</td>\n",
       "      <td>at</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2630</td>\n",
       "      <td>rural</td>\n",
       "      <td>B-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2631</td>\n",
       "      <td>health</td>\n",
       "      <td>I-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2632</td>\n",
       "      <td>centers</td>\n",
       "      <td>I-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2633</td>\n",
       "      <td>in</td>\n",
       "      <td>I-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2634</td>\n",
       "      <td>Africa</td>\n",
       "      <td>I-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2635</td>\n",
       "      <td>will</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2636</td>\n",
       "      <td>likely</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2637</td>\n",
       "      <td>decrease</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2638</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2639</td>\n",
       "      <td>Q2</td>\n",
       "      <td>B-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2640</td>\n",
       "      <td>2027</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2641</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2642</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2643</td>\n",
       "      <td>On</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2644</td>\n",
       "      <td>August</td>\n",
       "      <td>B-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2645</td>\n",
       "      <td>22</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2646</td>\n",
       "      <td>,</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2647</td>\n",
       "      <td>2025</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2648</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2649</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2650</td>\n",
       "      <td>American</td>\n",
       "      <td>B-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2651</td>\n",
       "      <td>Heart</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2652</td>\n",
       "      <td>Association</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2653</td>\n",
       "      <td>speculates</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>2654</td>\n",
       "      <td>that</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2655</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>2656</td>\n",
       "      <td>average</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2657</td>\n",
       "      <td>blood</td>\n",
       "      <td>I-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>2658</td>\n",
       "      <td>pressure</td>\n",
       "      <td>I-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>2659</td>\n",
       "      <td>levels</td>\n",
       "      <td>I-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2660</td>\n",
       "      <td>at</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>2661</td>\n",
       "      <td>urban</td>\n",
       "      <td>B-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>2662</td>\n",
       "      <td>hospitals</td>\n",
       "      <td>I-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2663</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2664</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2665</td>\n",
       "      <td>United</td>\n",
       "      <td>B-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2666</td>\n",
       "      <td>States</td>\n",
       "      <td>I-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2667</td>\n",
       "      <td>will</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>2668</td>\n",
       "      <td>likely</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2669</td>\n",
       "      <td>increase</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2670</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2671</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>2672</td>\n",
       "      <td>Dr.</td>\n",
       "      <td>B-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2673</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>2674</td>\n",
       "      <td>Kim</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2675</td>\n",
       "      <td>predicts</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>2676</td>\n",
       "      <td>that</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2677</td>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2678</td>\n",
       "      <td>January</td>\n",
       "      <td>B-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2679</td>\n",
       "      <td>10</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2680</td>\n",
       "      <td>,</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2681</td>\n",
       "      <td>2026</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2682</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2683</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>2684</td>\n",
       "      <td>number</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2685</td>\n",
       "      <td>of</td>\n",
       "      <td>I-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2686</td>\n",
       "      <td>people</td>\n",
       "      <td>I-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2687</td>\n",
       "      <td>practicing</td>\n",
       "      <td>I-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2688</td>\n",
       "      <td>yoga</td>\n",
       "      <td>I-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2689</td>\n",
       "      <td>will</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2690</td>\n",
       "      <td>likely</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2691</td>\n",
       "      <td>rise</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>2692</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2693</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>2694</td>\n",
       "      <td>According</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>2695</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>2696</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2697</td>\n",
       "      <td>Centers</td>\n",
       "      <td>B-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2698</td>\n",
       "      <td>for</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2699</td>\n",
       "      <td>Disease</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2700</td>\n",
       "      <td>Control</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2701</td>\n",
       "      <td>and</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2702</td>\n",
       "      <td>Prevention</td>\n",
       "      <td>I-p_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2703</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2704</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2705</td>\n",
       "      <td>obesity</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>2706</td>\n",
       "      <td>rates</td>\n",
       "      <td>I-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>2707</td>\n",
       "      <td>at</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>2708</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>B-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>2709</td>\n",
       "      <td>high</td>\n",
       "      <td>I-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2710</td>\n",
       "      <td>schools</td>\n",
       "      <td>I-p_t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>2711</td>\n",
       "      <td>will</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>2712</td>\n",
       "      <td>likely</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>2713</td>\n",
       "      <td>fall</td>\n",
       "      <td>B-p_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2714</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>2715</td>\n",
       "      <td>Q4</td>\n",
       "      <td>B-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>2716</td>\n",
       "      <td>2026</td>\n",
       "      <td>I-p_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>2717</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2718</td>\n",
       "      <td>dummy</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Index          Word POS Tag\n",
       "0     0.0                 dummy\n",
       "1    2504    Detravious   B-p_s\n",
       "2    2505             ,   I-p_s\n",
       "3    2506             a   I-p_s\n",
       "4    2507     financial   I-p_s\n",
       "5    2508       analyst   I-p_s\n",
       "6    2509     forecasts       O\n",
       "7    2510          that       O\n",
       "8    2511           the       O\n",
       "9    2512         stock   B-p_o\n",
       "10   2513         price   I-p_o\n",
       "11   2514            at       O\n",
       "12   2515        Amazon   B-p_t\n",
       "13   2516          will       O\n",
       "14   2517        likely       O\n",
       "15   2518      decrease   B-p_o\n",
       "16   2519            in       O\n",
       "17   2520          2027   B-p_d\n",
       "18   2521            Q3   I-p_d\n",
       "19   2522             .       O\n",
       "20   2523                      \n",
       "21   2524            On       O\n",
       "22   2525        August   B-p_d\n",
       "23   2526            21   I-p_d\n",
       "24   2527             ,   I-p_d\n",
       "25   2528          2024   I-p_d\n",
       "26   2529             ,       O\n",
       "27   2530       Goldman   B-p_s\n",
       "28   2531         Sachs   I-p_s\n",
       "29   2532    speculates       O\n",
       "30   2533          that       O\n",
       "31   2534           the       O\n",
       "32   2535     operating   B-p_o\n",
       "33   2536          cash   I-p_o\n",
       "34   2537          flow   I-p_o\n",
       "35   2538            at       O\n",
       "36   2539     Microsoft   B-p_t\n",
       "37   2540          will       O\n",
       "38   2541        likely       O\n",
       "39   2542      increase   B-p_o\n",
       "40   2543             .       O\n",
       "41   2544                      \n",
       "42   2545        Morgan   B-p_s\n",
       "43   2546       Stanley   I-p_s\n",
       "44   2547      predicts       O\n",
       "45   2548          that       O\n",
       "46   2549            on       O\n",
       "47   2550     September   B-p_d\n",
       "48   2551            15   I-p_d\n",
       "49   2552             ,   I-p_d\n",
       "50   2553          2025   I-p_d\n",
       "51   2554             ,       O\n",
       "52   2555           the       O\n",
       "53   2556           S&P   B-p_t\n",
       "54   2557           500   I-p_t\n",
       "55   2558         index   B-p_o\n",
       "56   2559          will       O\n",
       "57   2560        likely       O\n",
       "58   2561          rise   B-p_o\n",
       "59   2562             .       O\n",
       "60   2563                      \n",
       "61   2564     According       O\n",
       "62   2565            to       O\n",
       "63   2566         Apple   B-p_s\n",
       "64   2567             ,       O\n",
       "65   2568           the       O\n",
       "66   2569     projected       O\n",
       "67   2570       revenue   B-p_o\n",
       "68   2571            at       O\n",
       "69   2572      Alphabet   B-p_t\n",
       "70   2573          will       O\n",
       "71   2574        likely       O\n",
       "72   2575          fall   B-p_o\n",
       "73   2576            in       O\n",
       "74   2577            Q4   B-p_d\n",
       "75   2578          2026   I-p_d\n",
       "76   2579             .       O\n",
       "77   2580                      \n",
       "78   2581            In       O\n",
       "79   2582            Q2   B-p_d\n",
       "80   2583          2027   I-p_d\n",
       "81   2584             ,       O\n",
       "82   2585         Wells   B-p_s\n",
       "83   2586         Fargo   I-p_s\n",
       "84   2587       expects       O\n",
       "85   2588          that       O\n",
       "86   2589           the       O\n",
       "87   2590     inflation   B-p_o\n",
       "88   2591          rate   I-p_o\n",
       "89   2592            in       O\n",
       "90   2593           the       O\n",
       "91   2594            US   B-p_t\n",
       "92   2595          will       O\n",
       "93   2596        likely       O\n",
       "94   2597          stay       O\n",
       "95   2598        stable   B-p_o\n",
       "96   2599             .       O\n",
       "97   2600                      \n",
       "98   2601           The       O\n",
       "99   2602           Dow   B-p_t\n",
       "100  2603         Jones   I-p_t\n",
       "101  2604    Industrial   B-p_o\n",
       "102  2605       Average   I-p_o\n",
       "103  2606          will       O\n",
       "104  2607        likely       O\n",
       "105  2608          rise   B-p_o\n",
       "106  2609            in       O\n",
       "107  2610          2029   B-p_d\n",
       "108  2611             ,       O\n",
       "109  2612     according       O\n",
       "110  2613            to       O\n",
       "111  2614      JPMorgan   B-p_s\n",
       "112  2615         Chase   I-p_s\n",
       "113  2616             .       O\n",
       "114  2617                      \n",
       "115  2618           The       O\n",
       "116  2619         World   B-p_s\n",
       "117  2620        Health   I-p_s\n",
       "118  2621  Organization   I-p_s\n",
       "119  2622     forecasts       O\n",
       "120  2623          that       O\n",
       "121  2624           the       O\n",
       "122  2625    prevalence       O\n",
       "123  2626            of       O\n",
       "124  2627       chronic   B-p_o\n",
       "125  2628     illnesses   I-p_o\n",
       "126  2629            at       O\n",
       "127  2630         rural   B-p_t\n",
       "128  2631        health   I-p_t\n",
       "129  2632       centers   I-p_t\n",
       "130  2633            in   I-p_t\n",
       "131  2634        Africa   I-p_t\n",
       "132  2635          will       O\n",
       "133  2636        likely       O\n",
       "134  2637      decrease   B-p_o\n",
       "135  2638            in       O\n",
       "136  2639            Q2   B-p_d\n",
       "137  2640          2027   I-p_d\n",
       "138  2641             .       O\n",
       "139  2642                      \n",
       "140  2643            On       O\n",
       "141  2644        August   B-p_d\n",
       "142  2645            22   I-p_d\n",
       "143  2646             ,   I-p_d\n",
       "144  2647          2025   I-p_d\n",
       "145  2648             ,       O\n",
       "146  2649           the       O\n",
       "147  2650      American   B-p_s\n",
       "148  2651         Heart   I-p_s\n",
       "149  2652   Association   I-p_s\n",
       "150  2653    speculates       O\n",
       "151  2654          that       O\n",
       "152  2655           the       O\n",
       "153  2656       average   B-p_o\n",
       "154  2657         blood   I-p_o\n",
       "155  2658      pressure   I-p_o\n",
       "156  2659        levels   I-p_o\n",
       "157  2660            at       O\n",
       "158  2661         urban   B-p_t\n",
       "159  2662     hospitals   I-p_t\n",
       "160  2663            in       O\n",
       "161  2664           the       O\n",
       "162  2665        United   B-p_t\n",
       "163  2666        States   I-p_t\n",
       "164  2667          will       O\n",
       "165  2668        likely       O\n",
       "166  2669      increase   B-p_o\n",
       "167  2670             .       O\n",
       "168  2671                      \n",
       "169  2672           Dr.   B-p_s\n",
       "170  2673        Rachel   I-p_s\n",
       "171  2674           Kim   I-p_s\n",
       "172  2675      predicts       O\n",
       "173  2676          that       O\n",
       "174  2677            on       O\n",
       "175  2678       January   B-p_d\n",
       "176  2679            10   I-p_d\n",
       "177  2680             ,   I-p_d\n",
       "178  2681          2026   I-p_d\n",
       "179  2682             ,       O\n",
       "180  2683           the       O\n",
       "181  2684        number   B-p_o\n",
       "182  2685            of   I-p_o\n",
       "183  2686        people   I-p_o\n",
       "184  2687    practicing   I-p_o\n",
       "185  2688          yoga   I-p_o\n",
       "186  2689          will       O\n",
       "187  2690        likely       O\n",
       "188  2691          rise   B-p_o\n",
       "189  2692             .       O\n",
       "190  2693                      \n",
       "191  2694     According       O\n",
       "192  2695            to       O\n",
       "193  2696           the       O\n",
       "194  2697       Centers   B-p_s\n",
       "195  2698           for   I-p_s\n",
       "196  2699       Disease   I-p_s\n",
       "197  2700       Control   I-p_s\n",
       "198  2701           and   I-p_s\n",
       "199  2702    Prevention   I-p_s\n",
       "200  2703             ,       O\n",
       "201  2704           the       O\n",
       "202  2705       obesity   B-p_o\n",
       "203  2706         rates   I-p_o\n",
       "204  2707            at       O\n",
       "205  2708          U.S.   B-p_t\n",
       "206  2709          high   I-p_t\n",
       "207  2710       schools   I-p_t\n",
       "208  2711          will       O\n",
       "209  2712        likely       O\n",
       "210  2713          fall   B-p_o\n",
       "211  2714            in       O\n",
       "212  2715            Q4   B-p_d\n",
       "213  2716          2026   I-p_d\n",
       "214  2717             .       O\n",
       "215  2718         dummy   dummy"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dev_df.loc[1:1, \"Index\"] = \"2504\"\n",
    "updated_dev_df.loc[1:1, \"Word\"] = \"Detravious\"\n",
    "updated_dev_df.loc[1:1, \"POS Tag\"] = \"B-p_s\"\n",
    "\n",
    "updated_dev_df.loc[49:49, \"Index\"] = \"2552\"\n",
    "updated_dev_df.loc[49:49, \"Word\"] = \",\"\n",
    "updated_dev_df.loc[49:49, \"POS Tag\"] = \"I-p_d\"\n",
    "\n",
    "updated_dev_df.loc[50:50, \"Index\"] = \"2553\"\n",
    "updated_dev_df.loc[50:50, \"Word\"] = \"2025\"\n",
    "updated_dev_df.loc[50:50, \"POS Tag\"] = \"I-p_d\"\n",
    "\n",
    "updated_dev_df.loc[197:197, \"Index\"] = \"2700\"\n",
    "updated_dev_df.loc[197:197, \"Word\"] = \"Control\"\n",
    "updated_dev_df.loc[197:197, \"POS Tag\"] = \"I-p_s\"\n",
    "\n",
    "updated_dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_tags = updated_train_df['POS Tag'].unique()\n",
    "all_pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of Tasks\n",
    "\n",
    "1. Vocabulary Creation\n",
    "2. Model Learning\n",
    "3. Greedy Decoding with HMM\n",
    "4. Viterbi Decoding with HMM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Vocabulary Creation\n",
    "\n",
    "- **Problem:** Creating vocabulary to handle unkown words.\n",
    "    - **Solution:** Replace rare words wtih whose occurrences are less than a threshold (ie: 3) with a special token `< unk >`\n",
    "\n",
    "---\n",
    "\n",
    "1. [x] Create a vocabulary using the training data in the file train\n",
    "2. [x] Output the vocabulary into a txt file named `vocab.txt`\n",
    "    - [x] See PDF on how to properly format vocabulary file\n",
    "3. [x] Questions\n",
    "    1. [x] What is the selected threshold for unknown words replacement? 3\n",
    "    2. [x] What is the total size of your vocabulary? 13751\n",
    "    3. [x] What is the total occurrences of the special token `< unk >` after replacement? 29443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_false_series = updated_train_df['Word'].value_counts()\n",
    "vocab_df = pd.DataFrame(true_false_series)\n",
    "vocab_df.reset_index(inplace = True)\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_threshold_df(df: pd.DataFrame, word_col_name: str, count_col_name: str, threhold: int, special_token: str, save_df: bool, save_path_with_name: str):\n",
    "    \"\"\"For every word in df, replace with special_token if below threshold\n",
    "    \"\"\"\n",
    "    true_false_series = df[count_col_name] > 3\n",
    "    \n",
    "    updated_vocab_df = df.loc[true_false_series == True]\n",
    "    updated_false_vocab_df = df.loc[true_false_series == False]\n",
    "    updated_false_vocab_df[word_col_name] = special_token\n",
    "    \n",
    "    N_updated_false_vocab_df = len(updated_false_vocab_df)\n",
    "    \n",
    "    new_row = pd.DataFrame([[special_token, N_updated_false_vocab_df]], columns=updated_vocab_df.columns)\n",
    "    final_df = pd.concat([new_row, updated_vocab_df], ignore_index=True)\n",
    "    N_vocab = range(0, len(updated_vocab_df)+1)\n",
    "    \n",
    "    final_df[\"index\"] = N_vocab\n",
    "    \n",
    "    final_df = final_df.reindex(columns=[word_col_name, \"index\", count_col_name])\n",
    "    if save_df == True:\n",
    "        print(save_path_with_name)\n",
    "        final_df.to_csv(save_path_with_name, header=None, index=None, sep='\\t')\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_col_name = \"Word\"\n",
    "count_col_name = \"count\"\n",
    "special_token = \"< unk >\"\n",
    "save_df = False\n",
    "save_file_path_and_name = \"final_submit/vocab.txt\"\n",
    "updated_vocab_df = create_vocab_threshold_df(vocab_df, word_col_name, count_col_name, 3, special_token, save_df, save_file_path_and_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_vocab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Learning\n",
    "\n",
    "- **Main Idea**: Learn an HMM from the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **HMM Parameters:**\n",
    "\n",
    "  $$ \n",
    "  \n",
    "  Q = q_1 q_2 ... q_N \\text{, a set of N states} \\\\\n",
    "\n",
    "  O = o_1 o_2 ... o_T \\text{, a set of T observations}\\\\ \n",
    "  \n",
    "  $$\n",
    "\n",
    "  ---\n",
    "- **HMM Properties:**\n",
    "\n",
    "  $$\n",
    "\n",
    "  \\text{1. Markov Assumption:} P(q_i | q_1, ..., q_{i - 1}) = P(q_i | q_{i - 1})\\\\\n",
    "\n",
    "  \\text{2. Output Independence:} P(o_i | q_1, ..., q_i, ..., q_T, o_1, ..., o_i, ..., o_T) = P(o_i | q_i)\n",
    "\n",
    "  $$ \n",
    "  ---\n",
    "- **Mapping HMMs to Sequence Labelling:**\n",
    "\n",
    "  $$\n",
    "\n",
    "  q_i : o_i :: t_i : w_i \\\\\n",
    "\n",
    "  t_i, \\text{ tag } \\\\\n",
    "\n",
    "  w_i, \\text{ word} \n",
    "\n",
    "  $$\n",
    "  ---\n",
    "- **HMM Properties: wrt Sequence Labelling:**\n",
    "  $$\n",
    "  \\text{Transition Probability (} t \\text{)}: \\quad t(t_i \\mid t_{i - 1}) = \\frac{\\text{count}(t_{i - 1} \\rightarrow t_i)}{\\text{count}(t_{i - 1})}\n",
    "\n",
    "  \\\\\n",
    "\n",
    "  \\text{Emission Probability (} e \\text{)}: \\quad e(w_i \\mid t_i) = \\frac{\\text{count}(t_i \\rightarrow w_i)}{\\text{count}(t_i)}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "1. [x] Learn a model using the training data in the file train\n",
    "2. [x] Output the learned model into a model file in json format, named `hmm.json`. The model file should contains two dictionaries for the emission and transition parameters, respectively.\n",
    "    1. [x] 1st dictionary: Named transition, contains items with pairs of (s, s) as key and t(s|s) as value. \n",
    "    2. [x] 2nd dictionary: Named emission, contains items with pairs of (s, x) as key and e(x|s) as value.\n",
    "3. Question\n",
    "    1. [x] How many transition and emission parameters in your HMM? transition = 1416. emission = 50287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_train_df.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(df: pd.DataFrame, word_col_name: str, pos_tag_col_name: str, prev_pos_tag_col_name: str):\n",
    "    \"\"\"Count the transition and emission states, respectively\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The df to get the words and POS Tags from\n",
    "\n",
    "    word_col_name: `str`\n",
    "        The name of the word column in the df\n",
    "\n",
    "    pos_tag_col_name: `str`\n",
    "        The name of the POS Tag column in the df\n",
    "        \n",
    "    prev_pos_tag_col_name: `str`\n",
    "        The name of the Previous POS Tag column in the df\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    transition_states (`dict`), emission_state_word (`dict`), N_state (`dict`): `tuple`\n",
    "        A tuple with the counts for transition previous state and current state,\n",
    "        emission state and word, and total number of states\n",
    "    \n",
    "    \"\"\"\n",
    "    transition_states = defaultdict(int)\n",
    "    emission_state_word = defaultdict(int)\n",
    "    N_state = defaultdict(int)\n",
    "    \n",
    "    df[prev_pos_tag_col_name] = df[pos_tag_col_name].shift(1) # create new col to store previous states\n",
    "\n",
    "    vocabulary = df.iterrows()\n",
    "    # iterate through vocabulary\n",
    "    for _, row in tqdm(vocabulary, total=df.shape[0]):\n",
    "\n",
    "        emission_state_word[(row[pos_tag_col_name], row[word_col_name])] += 1 # get emissions count at POS Tag col and Word col\n",
    "        # transition count + 1\n",
    "        if pd.notnull(row[prev_pos_tag_col_name]):  # Check if it's not NaN\n",
    "            transition_states[(row[prev_pos_tag_col_name], row[pos_tag_col_name])] += 1 # get transition count at Previous POS Tag col and POS Tag col\n",
    "\n",
    "        \n",
    "        N_state[(row[pos_tag_col_name])] += 1 # increment POS Tag to get total number of states (POS Tags)\n",
    "\n",
    "    return transition_states, emission_state_word, N_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_col_name = \"Word\"\n",
    "pos_tag_col_name = \"POS Tag\"\n",
    "prev_pos_tag_col_name = 'Previous_POS Tag'\n",
    "transitions, emissions, N_states = get_counts(updated_train_df, word_col_name, pos_tag_col_name, prev_pos_tag_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"# Transition params = {len(transitions.items())} \\n# Emissions params = {len(emissions.items())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prob(transitions: dict, emissions: dict, N_states: dict, prob_type: str):   \n",
    "    \"\"\"Calculate the transistion and emissions probabilities, respectively\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transitions: `dict`\n",
    "        Counts for transition previous state and current state as key and value as total number (or counts) of pairs\n",
    "        \n",
    "    emissions: `dict`\n",
    "        Counts for emission state and word as key and value as total number (or counts) of pairs\n",
    "\n",
    "    N_states: `dict`\n",
    "        Counts of state (POS Tag) as key and value as total number (or counts) of states\n",
    "\n",
    "    prob_type: `str`\n",
    "        A string representing either transistion or emissions\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    store_probs: `dict`\n",
    "        A dictionary containing the probabilities of transitions and emissions, respectively. Key are pairings and values are probability, respectively\n",
    "    \"\"\"\n",
    "\n",
    "    if prob_type == \"t\":\n",
    "        t_or_e = transitions\n",
    "    elif prob_type == \"e\":\n",
    "        t_or_e = emissions\n",
    "    else:\n",
    "        print(f\"Invalid prob_type {prob_type}\")\n",
    "\n",
    "    store_probs = {}\n",
    "    for key, value in t_or_e.items():\n",
    "        \n",
    "        curr_state = key[0]       \n",
    "        store_probs[key] = value / N_states[curr_state]\n",
    "        \n",
    "    return store_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_probs = calculate_prob(transitions, emissions, N_states, 't')\n",
    "e_probs = calculate_prob(transitions, emissions, N_states, 'e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(t_probs.items())[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(e_probs.items())[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save HMM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_hmm = \"final_submit/hmm.json\"\n",
    "\n",
    "combine_t_and_e_probs = {}\n",
    "combine_t_and_e_probs[\"transitions\"] = t_probs\n",
    "combine_t_and_e_probs[\"emissions\"] = e_probs\n",
    "\n",
    "t_e_probs_df = pd.DataFrame(combine_t_and_e_probs)\n",
    "# t_e_probs_df.to_json(save_hmm) # save\n",
    "t_e_probs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Decoding Techniques:**\n",
    "    1. Greedy [find the optimal (OPT) solution at each step]\n",
    "    2. Viterbi [make use of dynammic programming to find the OPT solution with backtracking while searching the entire search space]\n",
    "4. **Notes of the data and given files:**\n",
    "    - Dataset: Wall Street Journal section of the Penn Treebank\n",
    "    - Folder named `data` with the following files:\n",
    "        1. `train`, sentences *with* human-annotated POS Tags\n",
    "        2. `dev`, sentences *with* human-annotated POS Tags\n",
    "        3. `test`, sentences *without* POS Tags, thus predict the POS Tags\n",
    "    - Format: Blank like at the end of each sentence. Each line contains 3 items separated by the `\\t`, the tab symbol. These three items are\n",
    "        1. Index of the word in the sentence\n",
    "        2. Word type\n",
    "        3. POS Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Greedy Decoding with HMM\n",
    "\n",
    "1. [x] Implement the greedy decoding algorithm\n",
    "2. [x] Evaluate it on the development data\n",
    "3. [x] Predicting the POS Tags of the sentences in the test data\n",
    "4. [x] Output the predictions in a file named `greedy.out`, in the same format of training data\n",
    "5. [x] Evaluate the results of the model on `eval.py` in the terminal with `python eval.py  p {predicted file}  g {gold-standard file}`\n",
    "    - Spefically: `python eval.py -p final_submit/greedy.out -g data/dev`\n",
    "6. [x] Question\n",
    "    1. [x] What is the accuracy on the dev data? 80.99%. Possibly need to properly clean data, improve Parts 1 and 2, and include more training data to improve accuracy. I didnt replace any words based on a certain threshold because I thought it was only for part 1. Some pairs (of both transition and emission, respectively) werent found, so I used a low number instead such that we dont pick that pair. I also need to learn how to write correct and efficient code. Seeing your solution to this HW and previous HWs will help as I struggled on all HWs thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dev_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(dev_df: pd.DataFrame, t_probs: dict, e_probs: dict, N_pos_tags: np.array):\n",
    "    \"\"\"Implement greedy decoding on the development file (words only) using the transition probability and emission probability. Furthermore, don't use POS Tag of development file, thus only use POS Tag from training data.\n",
    "\n",
    "    If 't_' or 'e_', transition and emission probabilities, respectively.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        Dev file\n",
    "\n",
    "    t_probs: `py dict`\n",
    "        Tranision probabilities for POS Tag given previous POS Tag\n",
    "\n",
    "    e_probs: `py dict`\n",
    "        Emission probabilities for Word given POS Tags\n",
    "\n",
    "    N_pos_tags: `np.array`\n",
    "        All POS Tags found in the training file\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    all_words_with_pos_tag: `list` \n",
    "        Store the words with highest probability POS Tag for that word as a tuple\n",
    "    \"\"\"\n",
    "\n",
    "    previous_pos_tag = \"dummy\"\n",
    "    not_found_value = 0.000000001\n",
    "    all_words_with_pos_tag = []\n",
    "\n",
    "    # Go through each row (word), get the corresponding POS Tag to calculate probabilities    \n",
    "    for index, row in tqdm(dev_df.iterrows(), total=dev_df.shape[0]):\n",
    "        # print(\"index\", index, \"with word\", row['Word'])\n",
    "\n",
    "        if row['POS Tag'] != \"dummy\": # check if POS Tag is dummy so we know where each new sentence starts\n",
    "\n",
    "            # For current word, store score from greedy calculatons. Empty when new word is encountered\n",
    "            store_scores = []\n",
    "            \n",
    "            for N_pos_tags_idx in range(len(N_pos_tags)):\n",
    "                current_pos_tag = N_pos_tags[N_pos_tags_idx]\n",
    "    \n",
    "                \"\"\"Transition Probability\n",
    "                    t(t_find_pos_tag | t_given_pos_tag)\n",
    "                \"\"\"\n",
    "                t_find_pos_tag = current_pos_tag\n",
    "                t_given_pos_tag = previous_pos_tag\n",
    "                \n",
    "                \"\"\"Emission Probability\n",
    "                    e(e_word | e_given_pos_tag)\n",
    "                \"\"\"\n",
    "                e_word = row['Word']\n",
    "                e_given_pos_tag = current_pos_tag\n",
    "                \n",
    "                \"\"\"Transition * Emission\"\"\"\n",
    "                t_key = (t_find_pos_tag, t_given_pos_tag)\n",
    "                e_key = (e_given_pos_tag, e_word)\n",
    "    \n",
    "                # IF-ELSE bc not all pairs will be found. If pair is found, use score, otherwise (pair isn't found) set alternative score\n",
    "                if t_key in t_probs and e_key in e_probs:\n",
    "                    t = t_probs[t_key]\n",
    "                    e = e_probs[e_key]\n",
    "                    score = t * e\n",
    "                    # print(f\"---  t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "                    \n",
    "                else:\n",
    "                    t = not_found_value\n",
    "                    e = not_found_value\n",
    "                    score = t * e\n",
    "                    # print(f\"--- t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "                            \n",
    "                store_scores.append(score)\n",
    "        \n",
    "            max_score_idx = np.argmax(np.array(store_scores)) # use argmax to get the index of max score\n",
    "            current_pos_tag = N_pos_tags[max_score_idx] # use the index of the max score to find which POS Tag to \n",
    "            all_words_with_pos_tag.append([row['Word'], current_pos_tag]) # store word and POS Tag with max score\n",
    "            previous_pos_tag = current_pos_tag # update the previous POS Tag\n",
    "        else:\n",
    "            empty = \"\" # formatting final 2D list\n",
    "            all_words_with_pos_tag.append([empty, empty]) # Adds extra space in final 2D list\n",
    "        \n",
    "    return all_words_with_pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_output = greedy_decoding(updated_dev_df, t_probs, e_probs, all_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_output = gd_output[1:] # remove intial empty list\n",
    "gd_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Greedy Decoding Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('final_submit/greedy.out', 'w') as op:\n",
    "    \n",
    "#     index = 1\n",
    "#     for idx, word in enumerate(gd_output):\n",
    "#         if word[0] == \"\":\n",
    "#             index = 1\n",
    "#             op.write(\"\\n\")\n",
    "#         else:\n",
    "#             op.write(f'{index}\\t{word[0]}\\t{word[1]}')\n",
    "#             op.write(\"\\n\")\n",
    "#             index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Viterbi Decoding with HMM\n",
    "\n",
    "1. [x] Implement the viterbi decoding algorithm\n",
    "2. [x] Evaluate it on the development data\n",
    "3. [x] Predict the POS Tags of the sentences in the test data\n",
    "4. [x] Output the predictions in a file named `viterbi.out`, in the same format of training data\n",
    "    - Specifically, `python eval.py -p final_submit/viterbi.out -g data/dev`\n",
    "5. [x] Question\n",
    "    1. [x] What is the accuracy on the dev data? 85.27%. Possibly need to properly clean data, improve Parts 1 and 2, and include more training data to improve accuracy. I didnt replace any words based on a certain threshold because I thought it was only for part 1. Some pairs (of both transition and emission, respectively) werent found, so I used a low number instead such that we dont pick that pair. I also need to learn how to write correct and efficient code. Seeing your solution to this HW and previous HWs will help as I struggled on all HWs thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat dev df so Viterbi will be more optimized compared to if dev df was a DF\n",
    "def dataframe_to_list(df: pd.DataFrame):\n",
    "    \"\"\"Convert a DF to a list of lists\"\"\"\n",
    "    list_of_sentences = []\n",
    "    sublist = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "\n",
    "        if row['POS Tag'] == 'dummy': # dummy POS Tag indicates a new sentence\n",
    "            list_of_sentences.append(sublist)\n",
    "            sublist = []\n",
    "        else:\n",
    "            sublist.append(row['Word'])\n",
    "            \n",
    "    # Append the last sublist\n",
    "    if sublist:\n",
    "        list_of_sentences.append(sublist)\n",
    "        \n",
    "    return list_of_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = dataframe_to_list(updated_dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Predict tag with Viterbi\n",
    "2. Pass to Sanket's code where we double check word has been seen before and has x tag his hashmap, so should/could be same for current--like a validation step\n",
    "3. If any words not in hashmap, pass to human-in-the-loop function by Sanket\n",
    "4. If unknown (under threshold) or dummy (for first tag), then pass to human-in-the-loop function by Sanket\n",
    "5. If any are wrong or confusions, human-in-the-loop function by Sanket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decoding(sentences: list, t_probs: dict, e_probs: dict, pos_tags: np.array):\n",
    "    \"\"\"Implement Viterbi decoding on the development file (words only) using the transition probability and emission probability. \n",
    "    \n",
    "    Parameters\n",
    "    ----------        \n",
    "    sentences: `list`\n",
    "        List of sentences from dev file\n",
    "\n",
    "    t_probs: `py dict`\n",
    "        Tranision probabilities for POS Tag given previous POS Tag\n",
    "\n",
    "    e_probs: `py dict`\n",
    "        Emission probabilities for Word given POS Tags\n",
    "\n",
    "    pos_tags: `np.array`\n",
    "        All POS Tags found in the training file\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    all_words_with_pos_tag: `list` \n",
    "        Store the words with highest probability POS Tag for that word as a tuple\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"Clarifications of variables\n",
    "        - If 't_' or 'e_', transition and emission probabilities, respectively.\n",
    "        - If `v_pi`, viterbi_pi (from slide deck as it had the pi symbol)\"\"\"\n",
    "    \"\"\"\n",
    "    Initialization with base cases\n",
    "        - For the first word of every new sentence, create a base case\n",
    "    \"\"\"\n",
    "    \n",
    "    initial_pos_tag = \"dummy\"\n",
    "    not_found_value = 0.000001\n",
    "    all_words_with_pos_tag = []\n",
    "    \n",
    "    for sentences_idx in range(len(sentences)):\n",
    "        sentence = sentences[sentences_idx]\n",
    "        print(f\"Sentence --- {sentence}\")\n",
    "\n",
    "        store_initial_scores = []\n",
    "\n",
    "        len_of_sentence = len(sentence)\n",
    "        N_pos_tags = len(pos_tags)\n",
    "        \n",
    "        v_pi = np.zeros((N_pos_tags, len_of_sentence)) # 2D matrix (or table) containing all POS tags and length of each specific sentence\n",
    "\n",
    "        for pos_tags_idx in range(N_pos_tags):\n",
    "            initial_t_given_pos_tag = initial_pos_tag\n",
    "            initial_t_find_pos_tag = pos_tags[pos_tags_idx]\n",
    "            initial_t_key = (initial_t_find_pos_tag, initial_t_given_pos_tag)\n",
    "            \n",
    "            initial_e_given_pos_tag = pos_tags[pos_tags_idx]\n",
    "            initial_e_word = sentence[0]\n",
    "            initial_e_key = (initial_e_given_pos_tag, initial_e_word)\n",
    "            \n",
    "            # Check if the keys for t_prob and e_prob are valid, respectively. If not, assign alternate score\n",
    "            if initial_t_key in t_probs and initial_e_key in e_probs:\n",
    "                v_pi[pos_tags_idx, 0] = t_probs[initial_t_key] * e_probs[initial_e_key]\n",
    "            else: \n",
    "                v_pi[pos_tags_idx, 0] = not_found_value\n",
    "        \n",
    "            store_initial_scores.append(v_pi[pos_tags_idx, 0])        \n",
    "        all_words_with_pos_tag.append([initial_e_word, pos_tags[pos_tags_idx]])\n",
    "\n",
    "\n",
    "        \"\"\"DP Algo\n",
    "            - End base case at first word this sentence\n",
    "            - For the remaining words in this sentence, find the best combo of word and POS Tag\n",
    "        \"\"\"\n",
    "        previous_word_idx = 0\n",
    "        \n",
    "        for word_idx in range(1, len_of_sentence):\n",
    "            e_word = sentence[word_idx]\n",
    "\n",
    "            word_with_best_pos_tags = []\n",
    "            \n",
    "            for pos_tags_idx in range(N_pos_tags):\n",
    "                current_pos_tag = pos_tags[pos_tags_idx]\n",
    "                \n",
    "                store_scores = []\n",
    "                \n",
    "                for previous_pos_tags_idx in range(N_pos_tags):\n",
    "                    previous_pos_tag = pos_tags[previous_pos_tags_idx]\n",
    "                    \n",
    "                    v_pi_idx = (previous_pos_tags_idx, previous_word_idx)\n",
    "                    \"\"\"Transition Probability\n",
    "                        t(t_find_pos_tag | t_given_pos_tag)\n",
    "                    \"\"\"\n",
    "                    t_find_pos_tag = current_pos_tag\n",
    "                    t_given_pos_tag = previous_pos_tag\n",
    "                    \n",
    "                    \n",
    "                    \"\"\"Emission Probability\n",
    "                        e(e_word | e_given_pos_tag)\n",
    "                    \"\"\"\n",
    "                    # e_word is above\n",
    "                    e_given_pos_tag = current_pos_tag \n",
    "                    \n",
    "                    \"\"\"Transition * Emission\"\"\"\n",
    "                    t_key = (t_find_pos_tag, t_given_pos_tag)\n",
    "                    e_key = (e_given_pos_tag, e_word)\n",
    "                    \n",
    "                    # IF-ELSE bc not all pairs will be found. If pair is found, use score, otherwise (pair isn't found) set alternative score\n",
    "                    if t_key in t_probs and e_key in e_probs:\n",
    "                        t = t_probs[t_key]\n",
    "                        e = e_probs[e_key]\n",
    "                        score = v_pi[v_pi_idx] * t * e\n",
    "                        # print(f\"--- FOUND: v_pi[{t_find_pos_tag}, {e_word}] = v_pi[{previous_pos_tag}, {previous_word}] * t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "\n",
    "                    else:\n",
    "                        t = not_found_value\n",
    "                        e = not_found_value\n",
    "                        score = v_pi[v_pi_idx] * t * e\n",
    "                        # print(f\"--- NOT FOUND: v_pi[{t_find_pos_tag}, {e_word}] = v_pi[{previous_pos_tag}, {previous_word}] * t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "\n",
    "                    store_scores.append(score)\n",
    "\n",
    "                max_score_idx = np.argmax(np.array(store_scores)) # use argmax to get the index of max score\n",
    "                current_pos_tag = pos_tags[max_score_idx] # use the index of the max score to find which POS Tag to update to\n",
    "                word_with_best_pos_tags.append(store_scores[max_score_idx]) # store max score \n",
    "            \n",
    "            max_score_of_word_idx = np.argmax(np.array(word_with_best_pos_tags))\n",
    "            all_words_with_pos_tag.append([e_word, pos_tags[max_score_of_word_idx]])\n",
    "            \n",
    "        empty = \"\" # formatting final 2D list\n",
    "        all_words_with_pos_tag.append([empty, empty]) # Adds extra space in final 2D list\n",
    "\n",
    "    return all_words_with_pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore first sentence as it's empty\n",
    "# ignore first tag as it's \"dummmy\"\n",
    "vd_output = viterbi_decoding(sentences[1:], t_probs, e_probs, all_pos_tags[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Viterbi Decoding Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('final_submit/viterbi.out', 'w') as op:\n",
    "#     # # # # # # # \n",
    "#     index = 1\n",
    "#     for idx, word in enumerate(vd_output):\n",
    "#         if word[0] == \"\":\n",
    "#             index = 1\n",
    "#             op.write(\"\\n\")\n",
    "#         else:\n",
    "#             op.write(f'{index}\\t{word[0]}\\t{word[1]}')\n",
    "#             op.write(\"\\n\")\n",
    "#             index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc\n",
    "\n",
    "- Learn an HMM from the training data\n",
    "\n",
    "- **HMM Parameters:**\n",
    "  $$ \n",
    "  Q = q_1 q_2 ... q_N \\\\\n",
    "  A = a_{11} ... a_{ij} ... a_{NN}, \\text{transition probability matrix } A \\\\\n",
    "    \\text{- probability of moving from state i to state j, s.t.} \\sum_{j = 1}^N a_{ij} = 1 \\forall i \\\\\n",
    "  B = b_i(o_t), \\text{emission probability} \\\\\n",
    "    \\text{- each expressing the probability of an observation } o_t (\\text{drawn from a vocabulary } V = v_1, v_2, ..., v_V) \\\\ \\text{being generated from a state } q_i \\\\\n",
    "\n",
    "\n",
    "  \\text{Transition Probability (} t \\text{)}: \\quad t(s' \\mid s) = \\frac{\\text{count}(s \\rightarrow s')}{\\text{count}(s)}\n",
    "\n",
    "  \\\\\n",
    "\n",
    "  \\text{Emission Probability (} e \\text{)}: \\quad e(x \\mid s) = \\frac{\\text{count}(s \\rightarrow x)}{\\text{count}(s)}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "1. [x] Learn a model using the training data in the file train\n",
    "2. [x] Output the learned model into a model file in json format, named `hmm.json`. The model file should contains two dictionaries for the emission and transition parameters, respectively.\n",
    "    1. [x] 1st dictionary: Named transition, contains items with pairs of (s, s) as key and t(s|s) as value. \n",
    "    2. [x] 2nd dictionary: Named emission, contains items with pairs of (s, x) as key and e(x|s) as value.\n",
    "3. Question\n",
    "    1. [x] How many transition and emission parameters in your HMM? transition = 1416. emission = 50287\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
