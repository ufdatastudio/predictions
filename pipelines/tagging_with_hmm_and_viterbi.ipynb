{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3-Part-of-Speech Tagging with HMMs + Decoding Techniques (Greedy and Viterbi)\n",
    "\n",
    "- Detravious Jamari Brinkley\n",
    "- CSCI-544: Applied Natural Language Processing\n",
    "- python version: 3\n",
    "\n",
    "---\n",
    "\n",
    "1. Part-of-Speech (POS) Tagging [a type of sequence labelling task where of a given word, assign the part of speech]\n",
    "2. HMMs (Hidden Markov Model) [a generative-based model that's used for POS Tagging]\n",
    "    1. Generative-based [provides the probabilities for all possible combinations of values of variables in the set using the joint distribution]\n",
    "    2. With POS Tagging: Given a sequence of observations (sentences), the task is to infer the most likely sequence of hidden states (POS Tags) that could have generated the observed data.\n",
    "3. **Decoding Techniques:**\n",
    "    1. Greedy [find the optimal (OPT) solution at each step]\n",
    "    2. Viterbi [make use of dynammic programming to find the OPT solution with backtracking while searching the entire search space]\n",
    "4. **Notes of the data and given files:**\n",
    "    - Dataset: Wall Street Journal section of the Penn Treebank\n",
    "    - Folder named `data` with the following files:\n",
    "        1. `train`, sentences *with* human-annotated POS Tags\n",
    "        2. `dev`, sentences *with* human-annotated POS Tags\n",
    "        3. `test`, sentences *without* POS Tags, thus predict the POS Tags\n",
    "    - Format: Blank like at the end of each sentence. Each line contains 3 items separated by the `\\t`, the tab symbol. These three items are\n",
    "        1. Index of the word in the sentence\n",
    "        2. Word type\n",
    "        3. POS Tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Update Data\n",
    "- [x] Find a way to separate sentences when loading the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str, file_name: str, is_test_file: bool, config_index: bool = True):\n",
    "    \n",
    "    if config_index == True:\n",
    "        if is_test_file != True:\n",
    "            file =  file_path + file_name\n",
    "            open_df = pd.read_table(file, sep = \"\\t\", names=['Index', 'Term', 'BIO x Prediction Tag'], skip_blank_lines=False)\n",
    "        else:\n",
    "            file =  file_path + file_name\n",
    "            open_df = pd.read_table(file, sep = \"\\t\", names=['Index', 'Word'], skip_blank_lines=False)\n",
    "        \n",
    "    return open_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df_rows_with_dummy(df: pd.DataFrame, new_columns_name: list) -> pd.DataFrame:  \n",
    "    \"\"\"Update the rows of the dataframe if blank space, fill with dummy\"\"\"  \n",
    "\n",
    "    dummy_row = pd.DataFrame([['0.0', ' ', 'dummy']], columns=df.columns)\n",
    "    df = pd.concat([dummy_row, df], ignore_index=True)\n",
    "    df.columns = new_columns_name\n",
    "    df.fillna(\"dummy\", inplace=True)\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data('../data/tagging/official/', 'train', False)\n",
    "# dev_df = load_data('data/', 'dev', False)\n",
    "# test_df = load_data('data/', 'test', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_columns_name = ['Index', 'Word', 'POS Tag']\n",
    "\n",
    "updated_train_df = update_df_rows_with_dummy(train_df, train_dev_columns_name)\n",
    "updated_dev_df = update_df_rows_with_dummy(dev_df, train_dev_columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_tags = updated_train_df['POS Tag'].unique()\n",
    "all_pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of Tasks\n",
    "\n",
    "1. Vocabulary Creation\n",
    "2. Model Learning\n",
    "3. Greedy Decoding with HMM\n",
    "4. Viterbi Decoding with HMM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Vocabulary Creation\n",
    "\n",
    "- **Problem:** Creating vocabulary to handle unkown words.\n",
    "    - **Solution:** Replace rare words wtih whose occurrences are less than a threshold (ie: 3) with a special token `< unk >`\n",
    "\n",
    "---\n",
    "\n",
    "1. [x] Create a vocabulary using the training data in the file train\n",
    "2. [x] Output the vocabulary into a txt file named `vocab.txt`\n",
    "    - [x] See PDF on how to properly format vocabulary file\n",
    "3. [x] Questions\n",
    "    1. [x] What is the selected threshold for unknown words replacement? 3\n",
    "    2. [x] What is the total size of your vocabulary? 13751\n",
    "    3. [x] What is the total occurrences of the special token `< unk >` after replacement? 29443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_false_series = updated_train_df['Word'].value_counts()\n",
    "vocab_df = pd.DataFrame(true_false_series)\n",
    "vocab_df.reset_index(inplace = True)\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_threshold_df(df: pd.DataFrame, word_col_name: str, count_col_name: str, threhold: int, special_token: str, save_df: bool, save_path_with_name: str):\n",
    "    \"\"\"For every word in df, replace with special_token if below threshold\n",
    "    \"\"\"\n",
    "    true_false_series = df[count_col_name] > 3\n",
    "    \n",
    "    updated_vocab_df = df.loc[true_false_series == True]\n",
    "    updated_false_vocab_df = df.loc[true_false_series == False]\n",
    "    updated_false_vocab_df[word_col_name] = special_token\n",
    "    \n",
    "    N_updated_false_vocab_df = len(updated_false_vocab_df)\n",
    "    \n",
    "    new_row = pd.DataFrame([[special_token, N_updated_false_vocab_df]], columns=updated_vocab_df.columns)\n",
    "    final_df = pd.concat([new_row, updated_vocab_df], ignore_index=True)\n",
    "    N_vocab = range(0, len(updated_vocab_df)+1)\n",
    "    \n",
    "    final_df[\"index\"] = N_vocab\n",
    "    \n",
    "    final_df = final_df.reindex(columns=[word_col_name, \"index\", count_col_name])\n",
    "    if save_df == True:\n",
    "        print(save_path_with_name)\n",
    "        final_df.to_csv(save_path_with_name, header=None, index=None, sep='\\t')\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_col_name = \"Word\"\n",
    "count_col_name = \"count\"\n",
    "special_token = \"< unk >\"\n",
    "save_df = False\n",
    "save_file_path_and_name = \"final_submit/vocab.txt\"\n",
    "updated_vocab_df = create_vocab_threshold_df(vocab_df, word_col_name, count_col_name, 3, special_token, save_df, save_file_path_and_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_vocab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Learning\n",
    "\n",
    "- Learn an HMM from the training data\n",
    "- **HMM Parameters:**\n",
    "  <div style=\"text-align: center;\">\n",
    "\n",
    "    $\n",
    "    \\text{Transition Probability (} t \\text{)}: \\quad t(s' \\mid s) = \\frac{\\text{count}(s \\rightarrow s')}{\\text{count}(s)}\n",
    "    $\n",
    "\n",
    "    $\n",
    "    \\text{Emission Probability (} e \\text{)}: \\quad e(x \\mid s) = \\frac{\\text{count}(s \\rightarrow x)}{\\text{count}(s)}\n",
    "    $\n",
    "\n",
    "  </div>\n",
    "\n",
    "---\n",
    "\n",
    "1. [x] Learn a model using the training data in the file train\n",
    "2. [x] Output the learned model into a model file in json format, named `hmm.json`. The model file should contains two dictionaries for the emission and transition parameters, respectively.\n",
    "    1. [x] 1st dictionary: Named transition, contains items with pairs of (s, s′) as key and t(s′|s) as value. \n",
    "    2. [x] 2nd dictionary: Named emission, contains items with pairs of (s, x) as key and e(x|s) as value.\n",
    "3. Question\n",
    "    1. [x] How many transition and emission parameters in your HMM? transition = 1416. emission = 50287\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(df: pd.DataFrame, word_col_name: str, pos_tag_col_name: str, prev_pos_tag_col_name: str):\n",
    "    \"\"\"Count the transition and emission states, respectively\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The df to get the words and POS Tags from\n",
    "\n",
    "    word_col_name: `str`\n",
    "        The name of the word column in the df\n",
    "\n",
    "    pos_tag_col_name: `str`\n",
    "        The name of the POS Tag column in the df\n",
    "        \n",
    "    prev_pos_tag_col_name: `str`\n",
    "        The name of the Previous POS Tag column in the df\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    transition_states (`dict`), emission_state_word (`dict`), N_state (`dict`): `tuple`\n",
    "        A tuple with the counts for transition previous state and current state,\n",
    "        emission state and word, and total number of states\n",
    "    \n",
    "    \"\"\"\n",
    "    transition_states = defaultdict(int)\n",
    "    emission_state_word = defaultdict(int)\n",
    "    N_state = defaultdict(int)\n",
    "    \n",
    "    df[prev_pos_tag_col_name] = df[pos_tag_col_name].shift(1) # create new col to store previous states\n",
    "\n",
    "    # iterate through vocabulary\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "\n",
    "        emission_state_word[(row[pos_tag_col_name], row[word_col_name])] += 1 # get emissions count at POS Tag col and Word col\n",
    "        # transition count + 1\n",
    "        if pd.notnull(row[prev_pos_tag_col_name]):  # Check if it's not NaN\n",
    "            transition_states[(row[prev_pos_tag_col_name], row[pos_tag_col_name])] += 1 # get transition count at Previous POS Tag col and POS Tag col\n",
    "\n",
    "        \n",
    "        N_state[(row[pos_tag_col_name])] += 1 # increment POS Tag to get total number of states (POS Tags)\n",
    "\n",
    "    return transition_states, emission_state_word, N_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_col_name = \"Word\"\n",
    "pos_tag_col_name = \"POS Tag\"\n",
    "prev_pos_tag_col_name = 'Previous_POS Tag'\n",
    "transitions, emissions, N_states = get_counts(updated_train_df, word_col_name, pos_tag_col_name, prev_pos_tag_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"# Transition params = {len(transitions.items())} \\n# Emissions params = {len(emissions.items())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prob(transitions: dict, emissions: dict, N_states: dict, prob_type: str):   \n",
    "    \"\"\"Calculate the transistion and emissions probabilities, respectively\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transitions: `dict`\n",
    "        Counts for transition previous state and current state as key and value as total number (or counts) of pairs\n",
    "        \n",
    "    emissions: `dict`\n",
    "        Counts for emission state and word as key and value as total number (or counts) of pairs\n",
    "\n",
    "    N_states: `dict`\n",
    "        Counts of state (POS Tag) as key and value as total number (or counts) of states\n",
    "\n",
    "    prob_type: `str`\n",
    "        A string representing either transistion or emissions\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    store_probs: `dict`\n",
    "        A dictionary containing the probabilities of transitions and emissions, respectively. Key are pairings and values are probability, respectively\n",
    "    \"\"\"\n",
    "\n",
    "    if prob_type == \"t\":\n",
    "        t_or_e = transitions\n",
    "    elif prob_type == \"e\":\n",
    "        t_or_e = emissions\n",
    "    else:\n",
    "        print(f\"Invalid prob_type {prob_type}\")\n",
    "\n",
    "    store_probs = {}\n",
    "    for key, value in t_or_e.items():\n",
    "        \n",
    "        curr_state = key[0]       \n",
    "        store_probs[key] = value / N_states[curr_state]\n",
    "        \n",
    "    return store_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_probs = calculate_prob(transitions, emissions, N_states, 't')\n",
    "e_probs = calculate_prob(transitions, emissions, N_states, 'e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(t_probs.items())[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(e_probs.items())[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save HMM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_hmm = \"final_submit/hmm.json\"\n",
    "\n",
    "combine_t_and_e_probs = {}\n",
    "combine_t_and_e_probs[\"transitions\"] = t_probs\n",
    "combine_t_and_e_probs[\"emissions\"] = e_probs\n",
    "\n",
    "t_e_probs_df = pd.DataFrame(combine_t_and_e_probs)\n",
    "# t_e_probs_df.to_json(save_hmm) # save\n",
    "t_e_probs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Greedy Decoding with HMM\n",
    "\n",
    "1. [x] Implement the greedy decoding algorithm\n",
    "2. [x] Evaluate it on the development data\n",
    "3. [x] Predicting the POS Tags of the sentences in the test data\n",
    "4. [x] Output the predictions in a file named `greedy.out`, in the same format of training data\n",
    "5. [x] Evaluate the results of the model on `eval.py` in the terminal with `python eval.py − p {predicted file} − g {gold-standard file}`\n",
    "    - Spefically: `python eval.py -p final_submit/greedy.out -g data/dev`\n",
    "6. [x] Question\n",
    "    1. [x] What is the accuracy on the dev data? 80.99%. Possibly need to properly clean data, improve Parts 1 and 2, and include more training data to improve accuracy. I didn’t replace any words based on a certain threshold because I thought it was only for part 1. Some pairs (of both transition and emission, respectively) weren’t found, so I used a low number instead such that we don’t pick that pair. I also need to learn how to write correct and efficient code. Seeing your solution to this HW and previous HWs will help as I struggled on all HWs thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_dev_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(dev_df: pd.DataFrame, t_probs: dict, e_probs: dict, N_pos_tags: np.array):\n",
    "    \"\"\"Implement greedy decoding on the development file (words only) using the transition probability and emission probability. Furthermore, don't use POS Tag of development file, thus only use POS Tag from training data.\n",
    "\n",
    "    If 't_' or 'e_', transition and emission probabilities, respectively.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        Dev file\n",
    "\n",
    "    t_probs: `py dict`\n",
    "        Tranision probabilities for POS Tag given previous POS Tag\n",
    "\n",
    "    e_probs: `py dict`\n",
    "        Emission probabilities for Word given POS Tags\n",
    "\n",
    "    N_pos_tags: `np.array`\n",
    "        All POS Tags found in the training file\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    all_words_with_pos_tag: `list` \n",
    "        Store the words with highest probability POS Tag for that word as a tuple\n",
    "    \"\"\"\n",
    "\n",
    "    previous_pos_tag = \"dummy\"\n",
    "    not_found_value = 0.000000001\n",
    "    all_words_with_pos_tag = []\n",
    "\n",
    "    # Go through each row (word), get the corresponding POS Tag to calculate probabilities    \n",
    "    for index, row in tqdm(dev_df.iterrows(), total=dev_df.shape[0]):\n",
    "        # print(\"index\", index, \"with word\", row['Word'])\n",
    "\n",
    "        if row['POS Tag'] != \"dummy\": # check if POS Tag is dummy so we know where each new sentence starts\n",
    "\n",
    "            # For current word, store score from greedy calculatons. Empty when new word is encountered\n",
    "            store_scores = []\n",
    "            \n",
    "            for N_pos_tags_idx in range(len(N_pos_tags)):\n",
    "                current_pos_tag = N_pos_tags[N_pos_tags_idx]\n",
    "    \n",
    "                \"\"\"Transition Probability\n",
    "                    t(t_find_pos_tag | t_given_pos_tag)\n",
    "                \"\"\"\n",
    "                t_find_pos_tag = current_pos_tag\n",
    "                t_given_pos_tag = previous_pos_tag\n",
    "                \n",
    "                \"\"\"Emission Probability\n",
    "                    e(e_word | e_given_pos_tag)\n",
    "                \"\"\"\n",
    "                e_word = row['Word']\n",
    "                e_given_pos_tag = current_pos_tag\n",
    "                \n",
    "                \"\"\"Transition * Emission\"\"\"\n",
    "                t_key = (t_find_pos_tag, t_given_pos_tag)\n",
    "                e_key = (e_given_pos_tag, e_word)\n",
    "    \n",
    "                # IF-ELSE bc not all pairs will be found. If pair is found, use score, otherwise (pair isn't found) set alternative score\n",
    "                if t_key in t_probs and e_key in e_probs:\n",
    "                    t = t_probs[t_key]\n",
    "                    e = e_probs[e_key]\n",
    "                    score = t * e\n",
    "                    # print(f\"---  t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "                    \n",
    "                else:\n",
    "                    t = not_found_value\n",
    "                    e = not_found_value\n",
    "                    score = t * e\n",
    "                    # print(f\"--- t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "                            \n",
    "                store_scores.append(score)\n",
    "        \n",
    "            max_score_idx = np.argmax(np.array(store_scores)) # use argmax to get the index of max score\n",
    "            current_pos_tag = N_pos_tags[max_score_idx] # use the index of the max score to find which POS Tag to \n",
    "            all_words_with_pos_tag.append([row['Word'], current_pos_tag]) # store word and POS Tag with max score\n",
    "            previous_pos_tag = current_pos_tag # update the previous POS Tag\n",
    "        else:\n",
    "            empty = \"\" # formatting final 2D list\n",
    "            all_words_with_pos_tag.append([empty, empty]) # Adds extra space in final 2D list\n",
    "        \n",
    "    return all_words_with_pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_output = greedy_decoding(updated_dev_df, t_probs, e_probs, all_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_output = gd_output[1:] # remove intial empty list\n",
    "gd_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Greedy Decoding Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('final_submit/greedy.out', 'w') as op:\n",
    "    \n",
    "#     index = 1\n",
    "#     for idx, word in enumerate(gd_output):\n",
    "#         if word[0] == \"\":\n",
    "#             index = 1\n",
    "#             op.write(\"\\n\")\n",
    "#         else:\n",
    "#             op.write(f'{index}\\t{word[0]}\\t{word[1]}')\n",
    "#             op.write(\"\\n\")\n",
    "#             index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Viterbi Decoding with HMM\n",
    "\n",
    "1. [x] Implement the viterbi decoding algorithm\n",
    "2. [x] Evaluate it on the development data\n",
    "3. [x] Predict the POS Tags of the sentences in the test data\n",
    "4. [x] Output the predictions in a file named `viterbi.out`, in the same format of training data\n",
    "    - Specifically, `python eval.py -p final_submit/viterbi.out -g data/dev`\n",
    "5. [x] Question\n",
    "    1. [x] What is the accuracy on the dev data? 85.27%. Possibly need to properly clean data, improve Parts 1 and 2, and include more training data to improve accuracy. I didn’t replace any words based on a certain threshold because I thought it was only for part 1. Some pairs (of both transition and emission, respectively) weren’t found, so I used a low number instead such that we don’t pick that pair. I also need to learn how to write correct and efficient code. Seeing your solution to this HW and previous HWs will help as I struggled on all HWs thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat dev df so Viterbi will be more optimized compared to if dev df was a DF\n",
    "def dataframe_to_list(df: pd.DataFrame):\n",
    "    \"\"\"Convert a DF to a list of lists\"\"\"\n",
    "    list_of_sentences = []\n",
    "    sublist = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "\n",
    "        if row['POS Tag'] == 'dummy': # dummy POS Tag indicates a new sentence\n",
    "            list_of_sentences.append(sublist)\n",
    "            sublist = []\n",
    "        else:\n",
    "            sublist.append(row['Word'])\n",
    "            \n",
    "    # Append the last sublist\n",
    "    if sublist:\n",
    "        list_of_sentences.append(sublist)\n",
    "        \n",
    "    return list_of_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = dataframe_to_list(updated_dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decoding(sentences: list, t_probs: dict, e_probs: dict, pos_tags: np.array):\n",
    "    \"\"\"Implement Viterbi decoding on the development file (words only) using the transition probability and emission probability. \n",
    "    \n",
    "    Parameters\n",
    "    ----------        \n",
    "    sentences: `list`\n",
    "        List of sentences from dev file\n",
    "\n",
    "    t_probs: `py dict`\n",
    "        Tranision probabilities for POS Tag given previous POS Tag\n",
    "\n",
    "    e_probs: `py dict`\n",
    "        Emission probabilities for Word given POS Tags\n",
    "\n",
    "    pos_tags: `np.array`\n",
    "        All POS Tags found in the training file\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    all_words_with_pos_tag: `list` \n",
    "        Store the words with highest probability POS Tag for that word as a tuple\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"Clarifications of variables\n",
    "        - If 't_' or 'e_', transition and emission probabilities, respectively.\n",
    "        - If `v_pi`, viterbi_pi (from slide deck as it had the pi symbol)\"\"\"\n",
    "    \"\"\"\n",
    "    Initialization with base cases\n",
    "        - For the first word of every new sentence, create a base case\n",
    "    \"\"\"\n",
    "    \n",
    "    initial_pos_tag = \"dummy\"\n",
    "    not_found_value = 0.000001\n",
    "    all_words_with_pos_tag = []\n",
    "    \n",
    "    for sentences_idx in range(len(sentences)):\n",
    "        sentence = sentences[sentences_idx]\n",
    "        # print(f\"Sentence --- {sentence}\")\n",
    "\n",
    "        store_initial_scores = []\n",
    "\n",
    "        len_of_sentence = len(sentence)\n",
    "        N_pos_tags = len(pos_tags)\n",
    "        \n",
    "        v_pi = np.zeros((N_pos_tags, len_of_sentence)) # 2D matrix (or table) containing all POS tags and length of each specific sentence\n",
    "\n",
    "        for pos_tags_idx in range(N_pos_tags):\n",
    "            initial_t_given_pos_tag = initial_pos_tag\n",
    "            initial_t_find_pos_tag = pos_tags[pos_tags_idx]\n",
    "            initial_t_key = (initial_t_find_pos_tag, initial_t_given_pos_tag)\n",
    "            \n",
    "            initial_e_given_pos_tag = pos_tags[pos_tags_idx]\n",
    "            initial_e_word = sentence[0]\n",
    "            initial_e_key = (initial_e_given_pos_tag, initial_e_word)\n",
    "            \n",
    "            # Check if the keys for t_prob and e_prob are valid, respectively. If not, assign alternate score\n",
    "            if initial_t_key in t_probs and initial_e_key in e_probs:\n",
    "                v_pi[pos_tags_idx, 0] = t_probs[initial_t_key] * e_probs[initial_e_key]\n",
    "            else: \n",
    "                v_pi[pos_tags_idx, 0] = not_found_value\n",
    "        \n",
    "            store_initial_scores.append(v_pi[pos_tags_idx, 0])        \n",
    "        all_words_with_pos_tag.append([initial_e_word, pos_tags[pos_tags_idx]])\n",
    "\n",
    "\n",
    "        \"\"\"DP Algo\n",
    "            - End base case at first word this sentence\n",
    "            - For the remaining words in this sentence, find the best combo of word and POS Tag\n",
    "        \"\"\"\n",
    "        previous_word_idx = 0\n",
    "        \n",
    "        for word_idx in range(1, len_of_sentence):\n",
    "            e_word = sentence[word_idx]\n",
    "\n",
    "            word_with_best_pos_tags = []\n",
    "            \n",
    "            for pos_tags_idx in range(N_pos_tags):\n",
    "                current_pos_tag = pos_tags[pos_tags_idx]\n",
    "                \n",
    "                store_scores = []\n",
    "                \n",
    "                for previous_pos_tags_idx in range(N_pos_tags):\n",
    "                    previous_pos_tag = pos_tags[previous_pos_tags_idx]\n",
    "                    \n",
    "                    v_pi_idx = (previous_pos_tags_idx, previous_word_idx)\n",
    "                    \"\"\"Transition Probability\n",
    "                        t(t_find_pos_tag | t_given_pos_tag)\n",
    "                    \"\"\"\n",
    "                    t_find_pos_tag = current_pos_tag\n",
    "                    t_given_pos_tag = previous_pos_tag\n",
    "                    \n",
    "                    \n",
    "                    \"\"\"Emission Probability\n",
    "                        e(e_word | e_given_pos_tag)\n",
    "                    \"\"\"\n",
    "                    # e_word is above\n",
    "                    e_given_pos_tag = current_pos_tag \n",
    "                    \n",
    "                    \"\"\"Transition * Emission\"\"\"\n",
    "                    t_key = (t_find_pos_tag, t_given_pos_tag)\n",
    "                    e_key = (e_given_pos_tag, e_word)\n",
    "                    \n",
    "                    # IF-ELSE bc not all pairs will be found. If pair is found, use score, otherwise (pair isn't found) set alternative score\n",
    "                    if t_key in t_probs and e_key in e_probs:\n",
    "                        t = t_probs[t_key]\n",
    "                        e = e_probs[e_key]\n",
    "                        score = v_pi[v_pi_idx] * t * e\n",
    "                        # print(f\"--- FOUND: v_pi[{t_find_pos_tag}, {e_word}] = v_pi[{previous_pos_tag}, {previous_word}] * t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "\n",
    "                    else:\n",
    "                        t = not_found_value\n",
    "                        e = not_found_value\n",
    "                        score = v_pi[v_pi_idx] * t * e\n",
    "                        # print(f\"--- NOT FOUND: v_pi[{t_find_pos_tag}, {e_word}] = v_pi[{previous_pos_tag}, {previous_word}] * t({t_find_pos_tag} | {t_given_pos_tag}) * e({e_word} | {e_given_pos_tag}) = {score}\")\n",
    "\n",
    "                    store_scores.append(score)\n",
    "\n",
    "                max_score_idx = np.argmax(np.array(store_scores)) # use argmax to get the index of max score\n",
    "                current_pos_tag = pos_tags[max_score_idx] # use the index of the max score to find which POS Tag to update to\n",
    "                word_with_best_pos_tags.append(store_scores[max_score_idx]) # store max score \n",
    "            \n",
    "            max_score_of_word_idx = np.argmax(np.array(word_with_best_pos_tags))\n",
    "            all_words_with_pos_tag.append([e_word, pos_tags[max_score_of_word_idx]])\n",
    "            \n",
    "        empty = \"\" # formatting final 2D list\n",
    "        all_words_with_pos_tag.append([empty, empty]) # Adds extra space in final 2D list\n",
    "\n",
    "    return all_words_with_pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore first sentence as it's empty\n",
    "# ignore first tag as it's \"dummmy\"\n",
    "vd_output = viterbi_decoding(sentences[1:], t_probs, e_probs, all_pos_tags[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Viterbi Decoding Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_submit/viterbi.out', 'w') as op:\n",
    "    # # # # # # # \n",
    "    index = 1\n",
    "    for idx, word in enumerate(vd_output):\n",
    "        if word[0] == \"\":\n",
    "            index = 1\n",
    "            op.write(\"\\n\")\n",
    "        else:\n",
    "            op.write(f'{index}\\t{word[0]}\\t{word[1]}')\n",
    "            op.write(\"\\n\")\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
