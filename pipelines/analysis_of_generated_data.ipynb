{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c6f57c",
   "metadata": {},
   "source": [
    "# Analysis of Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d042725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "print(notebook_dir)\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(os.path.join(notebook_dir, '../'))\n",
    "\n",
    "import log_files\n",
    "from data_processing import DataProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f09f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b9e62",
   "metadata": {},
   "source": [
    "## Load Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971bfee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = \"data/prediction_logs\"\n",
    "predictions = True\n",
    "predictions_df = log_files.read_data(notebook_dir, log_file_path, predictions)\n",
    "predictions_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b683200",
   "metadata": {},
   "source": [
    "## Load Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = \"data/observation_logs\"\n",
    "predictions = False\n",
    "observations_df = log_files.read_data(notebook_dir, log_file_path, predictions)\n",
    "observations_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c73034",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataProcessing.concat_dfs([predictions_df, observations_df])\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f4200",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b32ac",
   "metadata": {},
   "source": [
    "### Number of predictions and observations, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f460f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dict[\"#predictions\"] = len(predictions_df)\n",
    "analysis_dict[\"#observations\"] = len(observations_df)\n",
    "analysis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_counts(df: pd.DataFrame, analysis_dict: dict, type_data: str):\n",
    "    predictions_per_domain = df[\"Domain\"].value_counts()\n",
    "    analysis_dict[f\"#{type_data} per domain\"] = predictions_per_domain.to_dict()\n",
    "\n",
    "    predictions_per_model_name = df[\"Model Name\"].value_counts()\n",
    "    analysis_dict[f\"#{type_data} per model name\"] = predictions_per_model_name.to_dict()\n",
    "\n",
    "    predictions_per_api_name = df[\"API Name\"].value_counts()\n",
    "    analysis_dict[f\"#{type_data} per api name\"] = predictions_per_api_name.to_dict()\n",
    "\n",
    "    predictions_per_template_number = df[\"Template Number\"].value_counts()\n",
    "    analysis_dict[f\"#{type_data} per template #\"] = predictions_per_template_number.to_dict()\n",
    "\n",
    "    return analysis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_column_counts(predictions_df, analysis_dict, type_data=\"predictions\")\n",
    "get_column_counts(observations_df, analysis_dict, type_data=\"observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672af978",
   "metadata": {},
   "source": [
    "## Plot Data: Create + Save\n",
    "- Save dict after we create plots to ensure proper structure of dictionary. Proper structure as in we can extract contents to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38cf551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_versioned_run_directory(base_path):\n",
    "    \"\"\"\n",
    "    Creates a unique, versioned directory for a script run.\n",
    "    If 'path/to/analysis' exists, it will create 'path/to/analysis_v1', etc.\n",
    "    Returns the path of the directory that was created.\n",
    "    \"\"\"\n",
    "    # Sanitize base path by removing any trailing slash\n",
    "    if base_path.endswith('/') or base_path.endswith('\\\\'):\n",
    "        base_path = base_path[:-1]\n",
    "\n",
    "    # Start with the base path as the first candidate\n",
    "    run_directory = base_path\n",
    "    counter = 1\n",
    "    \n",
    "    # If the path already exists, start appending version numbers\n",
    "    while os.path.exists(run_directory):\n",
    "        run_directory = f\"{base_path}_v{counter}\"\n",
    "        counter += 1\n",
    "    \n",
    "    # Create the final, unique directory\n",
    "    os.makedirs(run_directory)\n",
    "    print(f\"Created output directory for this run: {run_directory}\\n\")\n",
    "    return run_directory\n",
    "\n",
    "def create_plots(results: dict, prediction_key: str, observation_key: str, save_data: bool, output_directory: str):\n",
    "    \"\"\"\n",
    "    Generates and saves a bar plot into the specified output_directory.\n",
    "    The directory is expected to already exist.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame([results[prediction_key], results[observation_key]], index=['Predictions', 'Observations'])\n",
    "    title = f\"{prediction_key} x {observation_key}\"\n",
    "    # Plot the DataFrame\n",
    "    df.plot(kind='bar', title=title, xlabel='Predictions/Observations', ylabel='Counts', rot=360)\n",
    "    \n",
    "    if save_data:\n",
    "        # The filename is now simple and doesn't need versioning.\n",
    "        image_filename = f\"{prediction_key}_vs_{observation_key}.png\".replace(\" \", \"_\").replace(\"#\", \"\")\n",
    "        \n",
    "        # The full path is simply the pre-determined directory + the filename.\n",
    "        full_path = os.path.join(output_directory, image_filename)\n",
    "        \n",
    "        plt.savefig(full_path, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {full_path}\\n\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "\n",
    "def save_json(obj, filepath):\n",
    "    \"\"\"\n",
    "    Saves a dictionary or other JSON-serializable object to a file.\n",
    "    Ensures the target directory exists before writing.\n",
    "    \"\"\"\n",
    "    # This line ensures the directory exists, which is helpful,\n",
    "    # though our main script already creates it.\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    \n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        # indent=4 makes the JSON file human-readable\n",
    "        json.dump(obj, f, indent=4)\n",
    "    print(f\"Data saved to: {filepath}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# Get today's date\n",
    "today = date.today()\n",
    "\n",
    "# Print the date\n",
    "print(\"Today's date:\", today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the base directory for all analysis runs.\n",
    "base_output_path = f\"../data/dataset_analyses/{today}\"\n",
    "\n",
    "# 2. Create ONE unique, versioned directory for THIS run.\n",
    "#    This function is called only once.\n",
    "run_output_directory = create_versioned_run_directory(base_output_path)\n",
    "\n",
    "create_plots(analysis_dict, \"#predictions\", \"#observations\", True, run_output_directory)\n",
    "create_plots(analysis_dict, \"#predictions per domain\", \"#observations per domain\", True, run_output_directory)\n",
    "create_plots(analysis_dict, \"#predictions per model name\", \"#observations per model name\", True, run_output_directory)\n",
    "create_plots(analysis_dict, \"#predictions per api name\", \"#observations per api name\", True, run_output_directory)\n",
    "create_plots(analysis_dict, \"#predictions per template #\", \"#observations per template #\", True, run_output_directory)\n",
    "\n",
    "json_filepath = os.path.join(run_output_directory, \"analysis_dict.json\")\n",
    "\n",
    "save_json(analysis_dict, json_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
