{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "125574b8",
   "metadata": {},
   "source": [
    "# Extract Features\n",
    "\n",
    "1. Read csv files and load as dfs\n",
    "2. Combine dfs\n",
    "3. Get semantic cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(os.path.join(notebook_dir, '../'))\n",
    "\n",
    "import log_files\n",
    "from log_files import LogData\n",
    "from data_processing import DataProcessing\n",
    "from feature_extraction import SpacyFeatureExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6fff7",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "- Use the structure from `1-generate_predictions-all_domains.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = \"data/prediction_logs\"\n",
    "predictions = True\n",
    "predictions_df = log_files.read_data(notebook_dir, log_file_path, predictions)\n",
    "predictions_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c376fa",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c874e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = \"data/observation_logs\"\n",
    "predictions = False\n",
    "observations_df = log_files.read_data(notebook_dir, log_file_path, predictions)\n",
    "observations_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1b2594",
   "metadata": {},
   "source": [
    "## Both\n",
    "\n",
    "- Create a knowledge graph\n",
    "    - Nodes: words\n",
    "    - Edges: connection to other words (same/diff sentence)\n",
    "- Look at code from Graphbreeding project on 2019 Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0713636",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataProcessing.concat_dfs([predictions_df, observations_df])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6a0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = DataProcessing.df_to_list(predictions_df, \"Base Sentence\")\n",
    "observations = DataProcessing.df_to_list(observations_df, \"Base Sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01271855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from text_generation_models import LlamaVersatileTextGenerationModel\n",
    "# llama_versatile_generation_model = LlamaVersatileTextGenerationModel()\n",
    "\n",
    "# from text_generation_models import TextGenerationModelFactory\n",
    "# tgmf = TextGenerationModelFactory()\n",
    "\n",
    "# # llama_versatile_generation_model = tgmf.create_instance(model_name='llama-3.3-70b-versatile')\n",
    "# # llama_instant_generation_model = tgmf.create_instance('llama-3.1-8b-instant')\n",
    "# llama_8b_8192_generation_model = tgmf.create_instance('llama3-8b-8192')\n",
    "\n",
    "\n",
    "# prompt = f\"Can you return the observations ({observations}) that certify this prediction ({predictions[0]})? Only write the observations that certify the prediction.\"\n",
    "# # prompt = f\"Can you return a list of the observations ({observations}) that certify this prediction ({predictions[0]}) and why? Only write the observations that certify the prediction and why. Do not write any other text. \"\n",
    "# input_prompt = llama_8b_8192_generation_model.user(prompt)\n",
    "# # print(input_prompt)\n",
    "# # raw_text = self.chat_completion([self.user(prompt_template)])\n",
    "# raw_text = llama_8b_8192_generation_model.chat_completion([input_prompt])\n",
    "# print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa63579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract observations from the string\n",
    "# observations = raw_text.split('\\n\\n')[1].split('\\n')[1:-1]\n",
    "# # Remove numbering and quotes\n",
    "# observations = [obs.split('. ', 1)[1].strip('\"') for obs in observations]\n",
    "# # Print the list\n",
    "# print(observations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341122b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7e0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de74d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4fd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from text_generation_models import TextGenerationModelFactory\n",
    "# tgmf = TextGenerationModelFactory()\n",
    "\n",
    "# llama_versatile_generation_model = tgmf.create_instance(model_name='llama-3.3-70b-versatile')\n",
    "# llama_instant_generation_model = tgmf.create_instance('llama-3.1-8b-instant')\n",
    "# llama_70b_8192_generation_model = tgmf.create_instance('llama3-70b-8192')\n",
    "# llama_8b_8192_generation_model = tgmf.create_instance('llama3-8b-8192')\n",
    "\n",
    "# models = [llama_70b_8192_generation_model]\n",
    "# # models = [llama_versatile_generation_model, llama_instant_generation_model, llama_70b_8192_generation_model, llama_8b_8192_generation_model]\n",
    "# # Prompt for the model\n",
    "\n",
    "# prompt = f\"Can you return the observations ({observations}) that certify this prediction ({predictions[0]})? Write in the format of ({predictions[0]}, {observations})\"\n",
    "# input_prompt = llama_versatile_generation_model.user(prompt)\n",
    "# # print(input_prompt)\n",
    "\n",
    "# model_certify = {}\n",
    "# for model in models:    \n",
    "    \n",
    "#     raw_text = model.chat_completion([input_prompt])\n",
    "#     print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29343c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540071b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# for model, output in model_certify.items():\n",
    "#     for o in output:\n",
    "#         data.append([model, o])\n",
    "\n",
    "# # Create the DataFrame\n",
    "# df = pd.DataFrame(data, columns=['Model', 'Output'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc367364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2824bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from text_generation_models import TextGenerationModelFactory\n",
    "\n",
    "# # Initialize the TextGenerationModelFactory\n",
    "# tgmf = TextGenerationModelFactory()\n",
    "\n",
    "# # Create instances of the models\n",
    "# llama_versatile_generation_model = tgmf.create_instance(model_name='llama-3.3-70b-versatile')\n",
    "# llama_instant_generation_model = tgmf.create_instance('llama-3.1-8b-instant')\n",
    "# llama_70b_8192_generation_model = tgmf.create_instance('llama3-70b-8192')\n",
    "# llama_8b_8192_generation_model = tgmf.create_instance('llama3-8b-8192')\n",
    "\n",
    "# # List of models\n",
    "# models = [llama_instant_generation_model, llama_70b_8192_generation_model, llama_8b_8192_generation_model]\n",
    "\n",
    "# # Prompt for the model\n",
    "# prompt = f\"Can you return a list of the observations ({observations}) that certify this prediction ({predictions[0]})? Only write the observations that certify the prediction. Do not write any other text.\"\n",
    "# input_prompt = llama_versatile_generation_model.user(prompt)\n",
    "\n",
    "# # Dictionary to store model outputs\n",
    "# model_certify = {}\n",
    "# for model in models:\n",
    "#     raw_text = model.chat_completion([input_prompt])\n",
    "#     output = [line.strip().replace(\"*\", \"\") for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "#     model_certify[model.model_name] = output\n",
    "\n",
    "# # Prepare data for DataFrame\n",
    "# data = []\n",
    "# for model, output in model_certify.items():\n",
    "#     for o in output:\n",
    "#         if isinstance(o, list):\n",
    "#             o = ', '.join(o)\n",
    "#         data.append([predictions[0], model, o])\n",
    "\n",
    "# # Create the DataFrame\n",
    "# df = pd.DataFrame(data, columns=['Prediction', 'Model', 'Observations'])\n",
    "\n",
    "# # Display the DataFrame\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39465f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84679fd",
   "metadata": {},
   "source": [
    "- Don't use only the {predictions[0]}, use the structure from `1-generate_prediction-all_domains.ipynb` and the spacy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "{predictions[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd80f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation_models import TextGenerationModelFactory\n",
    "tgmf = TextGenerationModelFactory()\n",
    "\n",
    "llama_versatile_generation_model = tgmf.create_instance(model_name='llama-3.3-70b-versatile')\n",
    "llama_instant_generation_model = tgmf.create_instance('llama-3.1-8b-instant')\n",
    "llama_70b_8192_generation_model = tgmf.create_instance('llama3-70b-8192')\n",
    "llama_8b_8192_generation_model = tgmf.create_instance('llama3-8b-8192')\n",
    "\n",
    "# models = [llama_instant_generation_model, llama_70b_8192_generation_model, llama_8b_8192_generation_model]\n",
    "models = [llama_instant_generation_model]\n",
    "\n",
    "# Prompt for the model\n",
    "\n",
    "prompt = f\"Return a list of the observations ({observations}) that certify this prediction ({predictions[0]})?\"\n",
    "input_prompt = llama_versatile_generation_model.user(prompt)\n",
    "# print(input_prompt)\n",
    "\n",
    "# df = pd.DataFrame(columns=[\"Model\", \"Prompt\", \"Response\"])\n",
    "model_certify = {}\n",
    "for model in models:    \n",
    "    \n",
    "    raw_text = model.chat_completion([input_prompt])\n",
    "    output = []\n",
    "    for line in raw_text.split(\"\\n\"):\n",
    "        if line.strip():  # Skip empty lines\n",
    "            output.append(line.strip())\n",
    "    # print(output)\n",
    "    model_certify[model.model_name] = output\n",
    "\n",
    "print(model_certify)\n",
    "model_certify.keys()\n",
    "\n",
    "data = []\n",
    "for model, output in model_certify.items():\n",
    "    for output in output:\n",
    "        data.append([model, output])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data, columns=['Model', 'Output'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e77823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec24fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = f\"Return top 5 list of the observations ({observations}) that certify this prediction ({predictions[0]})?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": prompt},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "chatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "chatbot(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e61b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
