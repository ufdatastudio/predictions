{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "125574b8",
   "metadata": {},
   "source": [
    "# Extract Features\n",
    "\n",
    "1. Read csv files and load as dfs\n",
    "2. Combine dfs\n",
    "3. Get semantic cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be1bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(os.path.join(notebook_dir, '../'))\n",
    "\n",
    "import log_files\n",
    "from log_files import LogData\n",
    "from data_processing import DataProcessing\n",
    "from feature_extraction import SpacyFeatureExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809d6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6fff7",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "- Use the structure from `1-generate_predictions-all_domains.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6be64ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start logging batch\n",
      "log_directory: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/prediction_logs\n",
      "save_batch_directory: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/prediction_logs/batch_1-prediction\n",
      "CSV to DF\n",
      "Load saved csv: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/prediction_logs/batch_1-prediction/batch_1-from_df.csv\n",
      "save_batch_directory: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/prediction_logs/batch_2-prediction\n",
      "CSV to DF\n",
      "Load saved csv: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/prediction_logs/batch_2-prediction/batch_2-from_df.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base Sentence</th>\n",
       "      <th>Sentence Label</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>API Name</th>\n",
       "      <th>Batch ID</th>\n",
       "      <th>Template Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JPMorgan Chase forecasts that the net profit at Amazon potentially decrease in Q3 of 2027.</td>\n",
       "      <td>1</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On August 21, 2024, Bank of America speculates the revenue at Microsoft will likely increase.</td>\n",
       "      <td>1</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Citigroup predicts on 2024-08-21, the operating income at Alphabet may rise.</td>\n",
       "      <td>1</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to Goldman Sachs, the research and development expenses at Facebook would fall in 2025.</td>\n",
       "      <td>1</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In 21 August 2024, Morgan Stanley envisions that the gross profit at Johnson &amp; Johnson has some probability to remain stable.</td>\n",
       "      <td>1</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The stock price at Visa should stay same in Q2 of 2026, according to Wells Fargo.</td>\n",
       "      <td>1</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JPMorgan forecasts that the revenue at Microsoft potentially decrease in Q3 of 2027.</td>\n",
       "      <td>1</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                   Base Sentence  \\\n",
       "0                                     JPMorgan Chase forecasts that the net profit at Amazon potentially decrease in Q3 of 2027.   \n",
       "1                                  On August 21, 2024, Bank of America speculates the revenue at Microsoft will likely increase.   \n",
       "2                                                   Citigroup predicts on 2024-08-21, the operating income at Alphabet may rise.   \n",
       "3                              According to Goldman Sachs, the research and development expenses at Facebook would fall in 2025.   \n",
       "4  In 21 August 2024, Morgan Stanley envisions that the gross profit at Johnson & Johnson has some probability to remain stable.   \n",
       "5                                              The stock price at Visa should stay same in Q2 of 2026, according to Wells Fargo.   \n",
       "6                                           JPMorgan forecasts that the revenue at Microsoft potentially decrease in Q3 of 2027.   \n",
       "\n",
       "   Sentence Label   Domain              Model Name    API Name  Batch ID  \\\n",
       "0               1  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "1               1  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "2               1  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "3               1  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "4               1  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "5               1  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "6               1  finance  llama-3.3-70b-instruct  NAVI_GATOR         0   \n",
       "\n",
       "   Template Number  \n",
       "0                1  \n",
       "1                2  \n",
       "2                3  \n",
       "3                4  \n",
       "4                5  \n",
       "5                6  \n",
       "6                1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_file_path = \"data/prediction_logs\"\n",
    "predictions = True\n",
    "predictions_df = log_files.read_data(notebook_dir, log_file_path, predictions)\n",
    "predictions_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c376fa",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c874e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start logging batch\n",
      "log_directory: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/observation_logs\n",
      "save_batch_directory: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/observation_logs/batch_1-observation\n",
      "CSV to DF\n",
      "Load saved csv: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/observation_logs/batch_1-observation/batch_1-from_df.csv\n",
      "save_batch_directory: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/observation_logs/batch_2-observation\n",
      "CSV to DF\n",
      "Load saved csv: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/observation_logs/batch_2-observation/batch_2-from_df.csv\n",
      "save_batch_directory: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/observation_logs/batch_3-observation\n",
      "CSV to DF\n",
      "Load saved csv: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/observation_logs/batch_3-observation/batch_3-from_df.csv\n",
      "save_batch_directory: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/observation_logs/batch_4-observation\n",
      "CSV to DF\n",
      "Load saved csv: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/observation_logs/batch_4-observation/batch_4-from_df.csv\n",
      "save_batch_directory: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/observation_logs/batch_5-observation\n",
      "CSV to DF\n",
      "Load saved csv: /Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../data/observation_logs/batch_5-observation/batch_5-from_df.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base Sentence</th>\n",
       "      <th>Sentence Label</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>API Name</th>\n",
       "      <th>Batch ID</th>\n",
       "      <th>Template Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The financial analyst at Goldman Sachs observed that the operating income at Tesla had increased in the first quarter of 2024.</td>\n",
       "      <td>0</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On 2024-08-20 to 2025-08-20, Morgan Stanley speculates the stock price at Amazon will likely rise.</td>\n",
       "      <td>0</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A young investor predicts on 2025-03-15, the S&amp;P 500 index may rise.</td>\n",
       "      <td>0</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to Bank of America, the net profit at Microsoft would fall in the second quarter of 2026.</td>\n",
       "      <td>0</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In 2027-01-01 to 2027-12-31, Wells Fargo envisions that the interest rates at the Federal Reserve have some probability to remain stable.</td>\n",
       "      <td>0</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The trading volume at Apple should stay same in the fourth quarter of 2025, according to a financial expert at JPMorgan Chase.</td>\n",
       "      <td>0</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JPMorgan observed that the net profit at Microsoft had risen in September 2023.</td>\n",
       "      <td>0</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                               Base Sentence  \\\n",
       "0             The financial analyst at Goldman Sachs observed that the operating income at Tesla had increased in the first quarter of 2024.   \n",
       "1                                         On 2024-08-20 to 2025-08-20, Morgan Stanley speculates the stock price at Amazon will likely rise.   \n",
       "2                                                                       A young investor predicts on 2025-03-15, the S&P 500 index may rise.   \n",
       "3                                        According to Bank of America, the net profit at Microsoft would fall in the second quarter of 2026.   \n",
       "4  In 2027-01-01 to 2027-12-31, Wells Fargo envisions that the interest rates at the Federal Reserve have some probability to remain stable.   \n",
       "5             The trading volume at Apple should stay same in the fourth quarter of 2025, according to a financial expert at JPMorgan Chase.   \n",
       "6                                                            JPMorgan observed that the net profit at Microsoft had risen in September 2023.   \n",
       "\n",
       "   Sentence Label   Domain              Model Name    API Name  Batch ID  \\\n",
       "0               0  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "1               0  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "2               0  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "3               0  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "4               0  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "5               0  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "6               0  finance  llama-3.3-70b-instruct  NAVI_GATOR         0   \n",
       "\n",
       "   Template Number  \n",
       "0                1  \n",
       "1                2  \n",
       "2                3  \n",
       "3                4  \n",
       "4                5  \n",
       "5                6  \n",
       "6                1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_file_path = \"data/observation_logs\"\n",
    "predictions = False\n",
    "observations_df = log_files.read_data(notebook_dir, log_file_path, predictions)\n",
    "observations_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1b2594",
   "metadata": {},
   "source": [
    "## Both\n",
    "\n",
    "- Create a knowledge graph\n",
    "    - Nodes: words\n",
    "    - Edges: connection to other words (same/diff sentence)\n",
    "- Look at code from Graphbreeding project on 2019 Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0713636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base Sentence</th>\n",
       "      <th>Sentence Label</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>API Name</th>\n",
       "      <th>Batch ID</th>\n",
       "      <th>Template Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JPMorgan Chase forecasts that the net profit at Amazon potentially decrease in Q3 of 2027.</td>\n",
       "      <td>1</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On August 21, 2024, Bank of America speculates the revenue at Microsoft will likely increase.</td>\n",
       "      <td>1</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Citigroup predicts on 2024-08-21, the operating income at Alphabet may rise.</td>\n",
       "      <td>1</td>\n",
       "      <td>finance</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>NAVI_GATOR</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   Base Sentence  \\\n",
       "0     JPMorgan Chase forecasts that the net profit at Amazon potentially decrease in Q3 of 2027.   \n",
       "1  On August 21, 2024, Bank of America speculates the revenue at Microsoft will likely increase.   \n",
       "2                   Citigroup predicts on 2024-08-21, the operating income at Alphabet may rise.   \n",
       "\n",
       "   Sentence Label   Domain              Model Name    API Name  Batch ID  \\\n",
       "0               1  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "1               1  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "2               1  finance  llama-3.1-70b-instruct  NAVI_GATOR         0   \n",
       "\n",
       "   Template Number  \n",
       "0                1  \n",
       "1                2  \n",
       "2                3  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DataProcessing.concat_dfs([predictions_df, observations_df])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e6a0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = DataProcessing.df_to_list(predictions_df, \"Base Sentence\")\n",
    "observations = DataProcessing.df_to_list(observations_df, \"Base Sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01271855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from text_generation_models import LlamaVersatileTextGenerationModel\n",
    "# llama_versatile_generation_model = LlamaVersatileTextGenerationModel()\n",
    "\n",
    "# from text_generation_models import TextGenerationModelFactory\n",
    "# tgmf = TextGenerationModelFactory()\n",
    "\n",
    "# # llama_versatile_generation_model = tgmf.create_instance(model_name='llama-3.3-70b-versatile')\n",
    "# # llama_instant_generation_model = tgmf.create_instance('llama-3.1-8b-instant')\n",
    "# llama_8b_8192_generation_model = tgmf.create_instance('llama3-8b-8192')\n",
    "\n",
    "\n",
    "# prompt = f\"Can you return the observations ({observations}) that certify this prediction ({predictions[0]})? Only write the observations that certify the prediction.\"\n",
    "# # prompt = f\"Can you return a list of the observations ({observations}) that certify this prediction ({predictions[0]}) and why? Only write the observations that certify the prediction and why. Do not write any other text. \"\n",
    "# input_prompt = llama_8b_8192_generation_model.user(prompt)\n",
    "# # print(input_prompt)\n",
    "# # raw_text = self.chat_completion([self.user(prompt_template)])\n",
    "# raw_text = llama_8b_8192_generation_model.chat_completion([input_prompt])\n",
    "# print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa63579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7843f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract observations from the string\n",
    "# observations = raw_text.split('\\n\\n')[1].split('\\n')[1:-1]\n",
    "# # Remove numbering and quotes\n",
    "# observations = [obs.split('. ', 1)[1].strip('\"') for obs in observations]\n",
    "# # Print the list\n",
    "# print(observations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "341122b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cfe5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7e0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de74d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de4fd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from text_generation_models import TextGenerationModelFactory\n",
    "# tgmf = TextGenerationModelFactory()\n",
    "\n",
    "# llama_versatile_generation_model = tgmf.create_instance(model_name='llama-3.3-70b-versatile')\n",
    "# llama_instant_generation_model = tgmf.create_instance('llama-3.1-8b-instant')\n",
    "# llama_70b_8192_generation_model = tgmf.create_instance('llama3-70b-8192')\n",
    "# llama_8b_8192_generation_model = tgmf.create_instance('llama3-8b-8192')\n",
    "\n",
    "# models = [llama_70b_8192_generation_model]\n",
    "# # models = [llama_versatile_generation_model, llama_instant_generation_model, llama_70b_8192_generation_model, llama_8b_8192_generation_model]\n",
    "# # Prompt for the model\n",
    "\n",
    "# prompt = f\"Can you return the observations ({observations}) that certify this prediction ({predictions[0]})? Write in the format of ({predictions[0]}, {observations})\"\n",
    "# input_prompt = llama_versatile_generation_model.user(prompt)\n",
    "# # print(input_prompt)\n",
    "\n",
    "# model_certify = {}\n",
    "# for model in models:    \n",
    "    \n",
    "#     raw_text = model.chat_completion([input_prompt])\n",
    "#     print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29343c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "540071b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# for model, output in model_certify.items():\n",
    "#     for o in output:\n",
    "#         data.append([model, o])\n",
    "\n",
    "# # Create the DataFrame\n",
    "# df = pd.DataFrame(data, columns=['Model', 'Output'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc367364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2824bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from text_generation_models import TextGenerationModelFactory\n",
    "\n",
    "# # Initialize the TextGenerationModelFactory\n",
    "# tgmf = TextGenerationModelFactory()\n",
    "\n",
    "# # Create instances of the models\n",
    "# llama_versatile_generation_model = tgmf.create_instance(model_name='llama-3.3-70b-versatile')\n",
    "# llama_instant_generation_model = tgmf.create_instance('llama-3.1-8b-instant')\n",
    "# llama_70b_8192_generation_model = tgmf.create_instance('llama3-70b-8192')\n",
    "# llama_8b_8192_generation_model = tgmf.create_instance('llama3-8b-8192')\n",
    "\n",
    "# # List of models\n",
    "# models = [llama_instant_generation_model, llama_70b_8192_generation_model, llama_8b_8192_generation_model]\n",
    "\n",
    "# # Prompt for the model\n",
    "# prompt = f\"Can you return a list of the observations ({observations}) that certify this prediction ({predictions[0]})? Only write the observations that certify the prediction. Do not write any other text.\"\n",
    "# input_prompt = llama_versatile_generation_model.user(prompt)\n",
    "\n",
    "# # Dictionary to store model outputs\n",
    "# model_certify = {}\n",
    "# for model in models:\n",
    "#     raw_text = model.chat_completion([input_prompt])\n",
    "#     output = [line.strip().replace(\"*\", \"\") for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "#     model_certify[model.model_name] = output\n",
    "\n",
    "# # Prepare data for DataFrame\n",
    "# data = []\n",
    "# for model, output in model_certify.items():\n",
    "#     for o in output:\n",
    "#         if isinstance(o, list):\n",
    "#             o = ', '.join(o)\n",
    "#         data.append([predictions[0], model, o])\n",
    "\n",
    "# # Create the DataFrame\n",
    "# df = pd.DataFrame(data, columns=['Prediction', 'Model', 'Observations'])\n",
    "\n",
    "# # Display the DataFrame\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c39465f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84679fd",
   "metadata": {},
   "source": [
    "- Don't use only the {predictions[0]}, use the structure from `1-generate_prediction-all_domains.ipynb` and the spacy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a07f4a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JPMorgan Chase forecasts that the net profit at Amazon potentially decrease in Q3 of 2027.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{predictions[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cd80f72",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m model_certify = {}\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:    \n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     raw_text = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_prompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     output = []\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m raw_text.split(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/research_labs/uf_ds/predictions/misc_experiments/../text_generation_models.py:156\u001b[39m, in \u001b[36mTextGenerationModelFactory.chat_completion\u001b[39m\u001b[34m(self, messages)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat_completion\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: List[Dict]) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    134\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate a chat completion response.\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    Parameters:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m \u001b[33;03m        The generated chat completion response.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/research_labs/uf_ds/predictions/.venv_predictions/lib/python3.11/site-packages/groq/resources/chat/completions.py:355\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    177\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    221\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    222\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    223\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    225\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    353\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    354\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/research_labs/uf_ds/predictions/.venv_predictions/lib/python3.11/site-packages/groq/_base_client.py:1222\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1209\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1210\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1217\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1218\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1219\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1220\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1221\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/research_labs/uf_ds/predictions/.venv_predictions/lib/python3.11/site-packages/groq/_base_client.py:1031\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1028\u001b[39m             err.response.read()\n\u001b[32m   1030\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "from text_generation_models import TextGenerationModelFactory\n",
    "tgmf = TextGenerationModelFactory()\n",
    "\n",
    "llama_versatile_generation_model = tgmf.create_instance(model_name='llama-3.3-70b-versatile')\n",
    "llama_instant_generation_model = tgmf.create_instance('llama-3.1-8b-instant')\n",
    "llama_70b_8192_generation_model = tgmf.create_instance('llama3-70b-8192')\n",
    "llama_8b_8192_generation_model = tgmf.create_instance('llama3-8b-8192')\n",
    "\n",
    "# models = [llama_instant_generation_model, llama_70b_8192_generation_model, llama_8b_8192_generation_model]\n",
    "models = [llama_instant_generation_model]\n",
    "\n",
    "# Prompt for the model\n",
    "\n",
    "prompt = f\"Return a list of the observations ({observations}) that certify this prediction ({predictions[0]})?\"\n",
    "input_prompt = llama_versatile_generation_model.user(prompt)\n",
    "# print(input_prompt)\n",
    "\n",
    "# df = pd.DataFrame(columns=[\"Model\", \"Prompt\", \"Response\"])\n",
    "model_certify = {}\n",
    "for model in models:    \n",
    "    \n",
    "    raw_text = model.chat_completion([input_prompt])\n",
    "    output = []\n",
    "    for line in raw_text.split(\"\\n\"):\n",
    "        if line.strip():  # Skip empty lines\n",
    "            output.append(line.strip())\n",
    "    # print(output)\n",
    "    model_certify[model.model_name] = output\n",
    "\n",
    "print(model_certify)\n",
    "model_certify.keys()\n",
    "\n",
    "data = []\n",
    "for model, output in model_certify.items():\n",
    "    for output in output:\n",
    "        data.append([model, output])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data, columns=['Model', 'Output'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e77823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
