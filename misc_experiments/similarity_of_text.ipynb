{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How similar is the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/detraviousjamaribrinkley/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, sys, spacy\n",
    "\n",
    "import pandas as pd\n",
    "# import gensim.downloader as api\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(os.path.join(notebook_dir, '../'))\n",
    "\n",
    "from feature_extraction import SpacyFeatureExtraction\n",
    "from data_processing import DataProcessing\n",
    "from clean_predictions import PredictionDataCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_nlp_model = spacy.load(\"en_core_web_md\")\n",
    "# pretrained_word_two_vec_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    \"I predict it will be sunny on 4 April 2025.\" : [\"It was sunny on 4 April 2025.\", \"It was not sunny on 4 April 2025.\"],\n",
    "    \"Apple's stock will rise by 10%.\" : [\"Apple's stock rose by 11%.\", \"Apple's stock rose by 900%.\", \"Apple's stock rose by 10%.\"]\n",
    "}\n",
    "\n",
    "mappings = {\n",
    "    \"Tesla's stock will rise by 10%.\" : [\"Apple's stock will rise by 10%.\", \"Apple's stock rose by 11%.\", \"Apple's stock rose by 10%.\", \"Apple's stock rose by 900%.\"]\n",
    "}\n",
    "\n",
    "mappings = {\n",
    "    \"Tesla's stock will rise by 10%.\" : [\"Apple's stock will rise by 10%.\", \"Apple's stock rose by 11%.\", \"Apple's stock rose by 10%.\", \"Apple's stock rose by 900%.\"],\n",
    "    \"Apple's stock will rise by 10%.\" : [\"Apple's stock will rise by 10%.\", \"Apple's stock rose by 11%.\", \"Apple's stock rose by 900%.\", \"Apple's stock rose by 10%.\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/9z0b45fx1xqbwxh8vk97lcfh0000gn/T/ipykernel_602/312081125.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = prediction_doc.similarity(observation_doc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Tesla's stock will rise by 10%.\n",
      "    Observation: Apple's stock will rise by 10%.\n",
      "    Similarity: 0.9796735644340515\n",
      "\n",
      "    Observation: Apple's stock rose by 11%.\n",
      "    Similarity: 0.8603723049163818\n",
      "\n",
      "    Observation: Apple's stock rose by 10%.\n",
      "    Similarity: 0.8691261410713196\n",
      "\n",
      "    Observation: Apple's stock rose by 900%.\n",
      "    Similarity: 0.8487552404403687\n",
      "\n",
      "Prediction: Apple's stock will rise by 10%.\n",
      "    Observation: Apple's stock will rise by 10%.\n",
      "    Similarity: 1.0\n",
      "\n",
      "    Observation: Apple's stock rose by 11%.\n",
      "    Similarity: 0.8647665977478027\n",
      "\n",
      "    Observation: Apple's stock rose by 900%.\n",
      "    Similarity: 0.8541026711463928\n",
      "\n",
      "    Observation: Apple's stock rose by 10%.\n",
      "    Similarity: 0.8756679892539978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_nlp_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "all_mappings = {} # Mapping of predictions to observations\n",
    "pom_mappings = {} # Mapping of predictions to observations to metrics/scoresl pos can be misleading\n",
    "\n",
    "oms = []\n",
    "for prediction in mappings.keys():\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "\n",
    "    om_mappings = {} # Mapping of observations to metrics/scores\n",
    "\n",
    "    for observation in mappings[prediction]:\n",
    "        print(f\"    Observation: {observation}\")\n",
    "\n",
    "        # Calculate the similarity score\n",
    "        prediction_doc = load_nlp_model(prediction)\n",
    "        observation_doc = load_nlp_model(observation)\n",
    "        similarity = prediction_doc.similarity(observation_doc)\n",
    "        print(f\"    Similarity: {similarity}\")\n",
    "        om_mappings[observation] = [similarity]\n",
    "        print()   \n",
    "        # print(f\"    OM Mapping: {om_mappings}\")\n",
    "    pom_mappings[prediction] = om_mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pom_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Observation</th>\n",
       "      <th>Spacy Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tesla's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock will rise by 10%.</td>\n",
       "      <td>0.979674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tesla's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock rose by 11%.</td>\n",
       "      <td>0.860372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tesla's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock rose by 10%.</td>\n",
       "      <td>0.869126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tesla's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock rose by 900%.</td>\n",
       "      <td>0.848755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock will rise by 10%.</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apple's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock rose by 11%.</td>\n",
       "      <td>0.864767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Apple's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock rose by 900%.</td>\n",
       "      <td>0.854103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Apple's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock rose by 10%.</td>\n",
       "      <td>0.875668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Prediction                      Observation  \\\n",
       "0  Tesla's stock will rise by 10%.  Apple's stock will rise by 10%.   \n",
       "1  Tesla's stock will rise by 10%.       Apple's stock rose by 11%.   \n",
       "2  Tesla's stock will rise by 10%.       Apple's stock rose by 10%.   \n",
       "3  Tesla's stock will rise by 10%.      Apple's stock rose by 900%.   \n",
       "4  Apple's stock will rise by 10%.  Apple's stock will rise by 10%.   \n",
       "5  Apple's stock will rise by 10%.       Apple's stock rose by 11%.   \n",
       "6  Apple's stock will rise by 10%.      Apple's stock rose by 900%.   \n",
       "7  Apple's stock will rise by 10%.       Apple's stock rose by 10%.   \n",
       "\n",
       "   Spacy Similarity  \n",
       "0          0.979674  \n",
       "1          0.860372  \n",
       "2          0.869126  \n",
       "3          0.848755  \n",
       "4          1.000000  \n",
       "5          0.864767  \n",
       "6          0.854103  \n",
       "7          0.875668  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the dictionary into a list of rows\n",
    "data = []\n",
    "for prediction, observations in pom_mappings.items():\n",
    "    for observation, scores in observations.items():\n",
    "        data.append([prediction, observation, scores[0]])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data, columns=['Prediction', 'Observation', 'Spacy Similarity'])\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison\n",
    "1. No focus on #s, more on words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/9z0b45fx1xqbwxh8vk97lcfh0000gn/T/ipykernel_602/1801851099.py:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = prediction_doc.similarity(observation_doc)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.925104558467865"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_doc = load_nlp_model(\"10%\")\n",
    "observation_doc = load_nlp_model(\"10.5%\")\n",
    "observation_doc_2 = load_nlp_model(\"11%\")\n",
    "similarity = prediction_doc.similarity(observation_doc)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/9z0b45fx1xqbwxh8vk97lcfh0000gn/T/ipykernel_602/4256136358.py:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = prediction_doc.similarity(observation_doc_2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9595414400100708"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = prediction_doc.similarity(observation_doc_2)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_doc = load_nlp_model(\"Apple's stock rose by 10%.\")\n",
    "# observation_doc = load_nlp_model(\"Apple's stock rose by 11%.\")\n",
    "# similarity = prediction_doc.similarity(observation_doc)\n",
    "# similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30040288, -0.46711448,  0.509174  ,  0.1586005 ,  1.1491344 ,\n",
       "       -0.8531876 ,  0.50995904,  0.27860662, -0.3254361 , -0.1560917 ,\n",
       "       -0.26071814,  0.29767483, -0.39653268,  1.1135507 , -0.2526727 ,\n",
       "        0.36130965, -0.81857663, -0.40923283, -1.0234685 ,  0.37592494,\n",
       "       -0.3877185 ,  0.78387195,  0.87653255,  0.03448987,  0.56956613,\n",
       "        0.1211684 ,  0.10060082, -0.39651704,  0.03264901,  0.2642506 ,\n",
       "        0.9213592 ,  0.03562765,  0.290687  , -0.16653627, -0.8325365 ,\n",
       "       -0.052293  , -0.7682663 , -0.4528045 ,  0.31796303,  0.5184007 ,\n",
       "       -0.36622745, -0.08055362, -0.50146323,  1.4582404 , -0.08100343,\n",
       "       -0.17221056, -0.84047353,  0.74705327, -0.02882719, -0.24370924,\n",
       "        0.57494605, -0.44427025,  0.3779632 , -0.6967993 ,  1.6423988 ,\n",
       "       -0.22711053,  0.22832489, -0.09112316,  0.3020852 ,  0.06474298,\n",
       "       -0.32058564, -0.47859693, -0.8530745 , -1.0867817 , -0.07193637,\n",
       "       -0.18350723, -0.02099572, -0.16591716, -0.6374334 ,  0.68538916,\n",
       "       -0.14947464,  0.5818212 ,  0.35669768, -0.35528103, -0.48867267,\n",
       "        0.197536  , -0.05889571, -0.5418419 , -0.35741997,  0.265813  ,\n",
       "        0.25137407, -0.44038785,  0.08692491,  0.07700702, -0.15524177,\n",
       "        0.28077337,  0.1793202 ,  0.2632093 , -0.14081793,  0.32976806,\n",
       "        0.22816056,  0.59694064, -0.19763334,  0.45102584, -0.02329731,\n",
       "       -0.03815684], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdv = prediction_doc.vector # 10\n",
    "pdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04759576, -0.3010687 ,  0.13733122,  0.09303613,  1.5409473 ,\n",
       "       -0.7031069 ,  0.42882633,  0.02008379,  0.05921438, -0.26620144,\n",
       "       -0.1433432 ,  0.22777021, -0.44027084,  1.2943797 , -0.02111381,\n",
       "        0.38246828, -0.65070444,  0.06801617, -0.87427586,  0.17526266,\n",
       "       -0.38326454,  1.0280204 ,  0.57325053, -0.11215457,  0.64881575,\n",
       "        0.2443167 , -0.21498092, -0.6615258 ,  0.28375006,  0.20735359,\n",
       "        0.81387967, -0.24033983,  0.37064815, -0.39679313, -0.9169711 ,\n",
       "        0.20031974, -0.4901891 , -0.23531848,  0.32470727,  0.6202681 ,\n",
       "        0.08728933, -0.25813505, -0.7733021 ,  1.5522578 ,  0.16665655,\n",
       "       -0.37733817, -0.8878894 ,  0.6666155 ,  0.16304165,  0.08384445,\n",
       "        0.19909966, -0.05296937,  0.3485108 , -0.93762916,  1.8119321 ,\n",
       "       -0.13450277,  0.20743313,  0.05026382,  0.3674773 ,  0.24333894,\n",
       "       -0.31718045, -0.64950943, -0.65276927, -0.98642814,  0.01255167,\n",
       "       -0.24843585, -0.31948543, -0.01021798, -0.77451   ,  0.41073936,\n",
       "       -0.2022182 ,  0.42515054,  0.09491271, -0.3805223 , -0.5405154 ,\n",
       "        0.10626293, -0.00455554, -0.84074116, -0.29556477, -0.14265332,\n",
       "        0.36548847, -0.5690372 , -0.11424978,  0.18303144, -0.27757204,\n",
       "        0.37690106,  0.5092439 , -0.04564411, -0.10898183,  0.5970289 ,\n",
       "        0.10195947,  0.63573736, -0.23857848,  0.27164304, -0.02935939,\n",
       "       -0.04821205], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odv_1 = observation_doc.vector # 10.5\n",
    "odv_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Five most popular similarity measures implementation in python](https://dataaspirant.com/five-most-popular-similarity-measures-implementation-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23161265, -0.43362677,  0.37865746,  0.12842351,  1.474714  ,\n",
       "       -0.74072057,  0.60628223,  0.2636813 , -0.17834927, -0.18636319,\n",
       "       -0.31557438,  0.19734126, -0.19305493,  1.1006242 , -0.0965907 ,\n",
       "        0.298085  , -0.93780375, -0.06959879, -0.84290695,  0.3828358 ,\n",
       "       -0.51774096,  1.064464  ,  0.7960936 ,  0.064152  ,  0.5760752 ,\n",
       "       -0.06326589, -0.01081962, -0.44413358,  0.12883908,  0.17736298,\n",
       "        0.7629027 , -0.13168155,  0.16614637, -0.35794282, -0.727747  ,\n",
       "       -0.08037841, -0.6158122 , -0.37415862,  0.30544767,  0.42032158,\n",
       "       -0.2694553 , -0.28181946, -0.7353734 ,  1.510866  ,  0.07865137,\n",
       "       -0.27992103, -0.8563335 ,  0.8413692 ,  0.03110853, -0.24775779,\n",
       "        0.36043683, -0.25667107,  0.33579728, -0.8342086 ,  1.9182353 ,\n",
       "        0.05007973,  0.3324153 ,  0.14967984,  0.43167794,  0.09882894,\n",
       "       -0.50839734, -0.63794184, -0.837104  , -0.95582104, -0.12988025,\n",
       "       -0.0932615 , -0.08784196, -0.10974275, -0.82919765,  0.36213374,\n",
       "        0.12850353,  0.6182556 ,  0.13620204, -0.122536  , -0.33855134,\n",
       "        0.06824949, -0.24348319, -0.8929744 , -0.30216312,  0.08082828,\n",
       "        0.2617958 , -0.33937138,  0.11949427,  0.3732591 , -0.24003285,\n",
       "        0.34914947,  0.24251698,  0.02941045, -0.22390746,  0.31599486,\n",
       "        0.14360946,  0.45739365, -0.23885584,  0.46346164, -0.02811906,\n",
       "       -0.2595338 ], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odv_2 = observation_doc_2.vector # 11\n",
    "odv_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pearson and Spearman Correlations\n",
    "    - 1 perfect positive relationship\n",
    "    - 0 no relationship\n",
    "    -  -1 perfect negative relationshp\n",
    "- Reference: https://mccormickml.com/2019/11/05/GLUE/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PearsonRResult(statistic=np.float32(0.9250324), pvalue=np.float64(2.722438055783015e-41)),\n",
       " PearsonRResult(statistic=np.float32(0.9595269), pvalue=np.float64(1.5838307052880837e-53)),\n",
       " SignificanceResult(statistic=np.float64(0.8928648941942485), pvalue=np.float64(2.4757606347787834e-34)),\n",
       " SignificanceResult(statistic=np.float64(0.9481551817688552), pvalue=np.float64(1.3813170610308815e-48)))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "p_result_1 = stats.pearsonr(pdv, odv_1)\n",
    "p_result_2 = stats.pearsonr(pdv, odv_2)\n",
    "\n",
    "s_result_1 = stats.spearmanr(pdv, odv_1)\n",
    "s_result_2 = stats.spearmanr(pdv, odv_2)\n",
    "\n",
    "p_result_1, p_result_2, s_result_1, s_result_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import*\n",
    "\n",
    "def square_rooted(x):\n",
    "\n",
    "    return round(sqrt(sum([a*a for a in x])),3)\n",
    "\n",
    "def cosine_similarity(x,y):\n",
    "\n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = square_rooted(x)*square_rooted(y)\n",
    "    return round(numerator/float(denominator),3)\n",
    "\n",
    "cosine_similarity(pdv, odv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_doc = load_nlp_model(\"10\")\n",
    "observation_doc = load_nlp_model(\"10.5\")\n",
    "observation_doc_2 = load_nlp_model(\"11\")\n",
    "\n",
    "similarity = prediction_doc.similarity(observation_doc)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = prediction_doc.similarity(observation_doc_2)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example sentences\n",
    "sentence1 = \"Apple's stock rose by 10%.\"\n",
    "sentence2 = \"Apple's stock rose by 10.5%.\"\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokens1 = tokenizer.tokenize(sentence1)\n",
    "tokens2 = tokenizer.tokenize(sentence2)\n",
    "\n",
    "# Add [CLS] and [SEP] tokens\n",
    "tokens = ['[CLS]'] + tokens1 + ['[SEP]'] + tokens2 + ['[SEP]']\n",
    "print(\"Token:\", tokens)\n",
    "\n",
    "# Convert tokens to input IDs\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Display the tokens and input IDs\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Convert tokens to input IDs\n",
    "input_ids1 = torch.tensor(tokenizer.convert_tokens_to_ids(tokens1)).unsqueeze(0)  # Batch size 1\n",
    "input_ids2 = torch.tensor(tokenizer.convert_tokens_to_ids(tokens2)).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "# Obtain the BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(input_ids1)\n",
    "    outputs2 = model(input_ids2)\n",
    "    embeddings1 = outputs1.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "    embeddings2 = outputs2.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "\n",
    "# Calculate similarity\n",
    "similarity_score = cosine_similarity(embeddings1, embeddings2)\n",
    "print(\"Similarity Score:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    \"Prediction\": [\"I am predicting it will be sunny on 4 April 2025.\", \"Apple's stock will be rising by 10%.\", \"Apple's stock will be rising by 10%.\"],\n",
    "    \"Observation\": [\"It was sunny on 4 April 2025\", \"Apple's stock is rising by 900%.\",  \"Apple's stock rose by 900%.\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(mappings)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "clean_data = PredictionDataCleaner(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_predictions_df = clean_data.lemmatize(col_name='Prediction', lem_col_name='Predictions Lem')\n",
    "updated_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_observations_df = clean_data.lemmatize(col_name='Observation', lem_col_name='Observations Lem')\n",
    "updated_observations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_df = updated_predictions_df.merge(updated_observations_df)\n",
    "lem_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Issues above are: \n",
    "\n",
    "    1. \"'s\" isn't joined to Apple\n",
    "    2. It doesn't consider \"will be rising\" as a single future tense phrase, thus breaks it up into words (with only one future tense word of \"rising\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem.lemmatize('predicting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem.lemmatize('rising')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem.lemmatize('rocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"predicting, predicts, rising, rises, rocks, rose, will be rising, is rising by.\")\n",
    "doc = nlp(\"I am predicting it will be sunny on 4 April 2025.\")\n",
    "\n",
    "words_lemmas_list = [token.lemma_ for token in doc]\n",
    "print(words_lemmas_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
