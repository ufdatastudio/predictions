{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTas9ZQ7lXP7"
   },
   "source": [
    "# Tutorial: Creating a Hybrid Retrieval Pipeline\n",
    "\n",
    "- **Level**: Intermediate\n",
    "- **Time to complete**: 15 minutes\n",
    "- **Components Used**: [`DocumentSplitter`](https://docs.haystack.deepset.ai/docs/documentsplitter), [`SentenceTransformersDocumentEmbedder`](https://docs.haystack.deepset.ai/docs/sentencetransformersdocumentembedder), [`DocumentJoiner`](https://docs.haystack.deepset.ai/docs/documentjoiner), [`InMemoryDocumentStore`](https://docs.haystack.deepset.ai/docs/inmemorydocumentstore), [`InMemoryBM25Retriever`](https://docs.haystack.deepset.ai/docs/inmemorybm25retriever), [`InMemoryEmbeddingRetriever`](https://docs.haystack.deepset.ai/docs/inmemoryembeddingretriever), and [`TransformersSimilarityRanker`](https://docs.haystack.deepset.ai/docs/transformerssimilarityranker)\n",
    "- **Prerequisites**: None\n",
    "- **Goal**: After completing this tutorial, you will have learned about creating a hybrid retrieval and when it's useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(os.path.join(notebook_dir, '../'))\n",
    "\n",
    "# import log_files\n",
    "from data_processing import DataProcessing\n",
    "from text_generation_models import TextGenerationModelFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hw_zoKolXQL"
   },
   "source": [
    "## Overview\n",
    "\n",
    "**Hybrid Retrieval** combines keyword-based and embedding-based retrieval techniques, leveraging the strengths of both approaches. In essence, dense embeddings excel in grasping the contextual nuances of the query, while keyword-based methods excel in matching keywords.\n",
    "\n",
    "There are many cases when a simple keyword-based approaches like BM25 performs better than a dense retrieval (for example in a specific domain like healthcare) because a dense model needs to be trained on data. For more details about Hybrid Retrieval, check out [Blog Post: Hybrid Document Retrieval](https://haystack.deepset.ai/blog/hybrid-retrieval)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITs3WTT5lXQT"
   },
   "source": [
    "## Preparing the Colab Environment\n",
    "\n",
    "- [Enable GPU Runtime in Colab](https://docs.haystack.deepset.ai/docs/enabling-gpu-acceleration)\n",
    "- [Set logging level to INFO](https://docs.haystack.deepset.ai/docs/setting-the-log-level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2g9fhjxDlXQb"
   },
   "source": [
    "## Installing Haystack\n",
    "\n",
    "Install Haystack and other required packages with `pip`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usdANiAGlXQ9"
   },
   "source": [
    "## Initializing the DocumentStore\n",
    "\n",
    "You'll start creating your question answering system by initializing a DocumentStore. A DocumentStore stores the Documents that your system uses to find answers to your questions. In this tutorial, you'll be using the [`InMemoryDocumentStore`](https://docs.haystack.deepset.ai/docs/inmemorydocumentstore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cLbh-UtelXRL"
   },
   "outputs": [],
   "source": [
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZWBHcc8TKcv"
   },
   "source": [
    "> `InMemoryDocumentStore` is the simplest DocumentStore to get started with. It requires no external dependencies and it's a good option for smaller projects and debugging. But it doesn't scale up so well to larger Document collections, so it's not a good choice for production systems. To learn more about the different types of external databases that Haystack supports, see [DocumentStore Integrations](https://haystack.deepset.ai/integrations?type=Document+Store&version=2.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rk8fdMzTb-I"
   },
   "source": [
    "## Fetching and Processing Documents\n",
    "\n",
    "As Documents, you will use the PubMed Abstracts. There are a lot of datasets from PubMed on Hugging Face Hub; you will use [anakin87/medrag-pubmed-chunk](https://huggingface.co/datasets/anakin87/medrag-pubmed-chunk) in this tutorial.\n",
    "\n",
    "Then, you will create Documents from the dataset with a simple for loop.\n",
    "Each data point in the PubMed dataset has 4 features:\n",
    "* *pmid*\n",
    "* *title*\n",
    "* *content*: the abstract\n",
    "* *contents*: abstract + title\n",
    "\n",
    "For searching, you will use the *contents* feature. The other features will be stored as metadata, and you will use them to have a **pretty print** of the search results or for [metadata filtering](https://docs.haystack.deepset.ai/docs/metadata-filtering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvrG_QzirSsq"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from haystack import Document\n",
    "\n",
    "# Get original dataset\n",
    "financial_full_path = os.path.join(notebook_dir, '../data/', 'financial_phrase_bank/all_data-adjusted_header.csv')\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=financial_full_path, split=\"train\")\n",
    "\n",
    "docs = []\n",
    "\n",
    "for doc in dataset:\n",
    "    docs.append(\n",
    "        Document(content=doc[\"sentence\"], meta={\"sentiment\": doc[\"sentiment\"]})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = docs[:33]\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPngNEs5q8Tw"
   },
   "source": [
    "## Indexing Documents with a Pipeline\n",
    "\n",
    "Create a pipeline to store the data in the document store with their embedding. For this pipeline, you need a [DocumentSplitter](https://docs.haystack.deepset.ai/docs/documentsplitter) to split documents into chunks of 512 words, [SentenceTransformersDocumentEmbedder](https://docs.haystack.deepset.ai/docs/sentencetransformersdocumentembedder) to create document embeddings for dense retrieval and [DocumentWriter](https://docs.haystack.deepset.ai/docs/documentwriter) to write documents to the document store.\n",
    "\n",
    "As an embedding model, you will use [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) on Hugging Face. Feel free to test other models on Hugging Face or use another [Embedder](https://docs.haystack.deepset.ai/docs/embedders) to switch the model provider.\n",
    "\n",
    "> If this step takes too long for you, replace the embedding model with a smaller model such as `sentence-transformers/all-MiniLM-L6-v2` or `sentence-transformers/all-mpnet-base-v2`. Make sure that the `split_length` is updated according to your model's token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RrIN83JNCHhX"
   },
   "outputs": [],
   "source": [
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.preprocessors.document_splitter import DocumentSplitter\n",
    "from haystack import Pipeline\n",
    "from haystack.utils import ComponentDevice\n",
    "\n",
    "document_splitter = DocumentSplitter(split_by=\"word\", split_length=512, split_overlap=32)\n",
    "document_embedder = SentenceTransformersDocumentEmbedder(\n",
    "    model=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "document_writer = DocumentWriter(document_store)\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"document_splitter\", document_splitter) # sentences/documents alone\n",
    "indexing_pipeline.add_component(\"document_embedder\", document_embedder) # embeddings per document\n",
    "indexing_pipeline.add_component(\"document_writer\", document_writer) # documents -> vector store\n",
    "\n",
    "indexing_pipeline.connect(\"document_splitter\", \"document_embedder\") # map documents : embeddings\n",
    "indexing_pipeline.connect(\"document_embedder\", \"document_writer\") # embeddings : vector store\n",
    "\n",
    "indexing_pipeline.run({\"document_splitter\": {\"documents\": docs}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSJlHxrhgQby"
   },
   "source": [
    "Documents are stored in `InMemoryDocumentStore` with their embeddings, now it's time for creating the hybrid retrieval pipeline âœ…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgMgY-d9najg"
   },
   "source": [
    "## Creating a Pipeline for Hybrid Retrieval\n",
    "\n",
    "Hybrid retrieval refers to the combination of multiple retrieval methods to enhance overall performance. In the context of search systems, a hybrid retrieval pipeline executes both traditional keyword-based search and dense vector search, later ranking the results with a **cross-encoder model**. This combination allows the search system to leverage the strengths of different approaches, providing more accurate and diverse results.\n",
    "\n",
    "Here are the required steps for a hybrid retrieval pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ha8pNmnvqj4n"
   },
   "source": [
    "### 1) Initialize Retrievers and the Embedder\n",
    "\n",
    "Initialize a [InMemoryEmbeddingRetriever](https://docs.haystack.deepset.ai/docs/inmemoryembeddingretriever) and [InMemoryBM25Retriever](https://docs.haystack.deepset.ai/docs/inmemorybm25retriever) to perform both dense and keyword-based retrieval. For dense retrieval, you also need a [SentenceTransformersTextEmbedder](https://docs.haystack.deepset.ai/docs/sentencetransformerstextembedder) that computes the embedding of the search query by using the same embedding model `BAAI/bge-small-en-v1.5` that was used in the indexing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DVfQvnWYrMWr"
   },
   "outputs": [],
   "source": [
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever, InMemoryEmbeddingRetriever\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "\n",
    "text_embedder = SentenceTransformersTextEmbedder(\n",
    "    model=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "embedding_retriever = InMemoryEmbeddingRetriever(document_store)\n",
    "bm25_retriever = InMemoryBM25Retriever(document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FC81c8RBrRFf"
   },
   "source": [
    "### 2) Join Retrieval Results\n",
    "\n",
    "Haystack offers several joining methods in [`DocumentJoiner`](https://docs.haystack.deepset.ai/docs/documentjoiner) to be used for different use cases such as `merge` and `reciprocal_rank_fusion`. In this example, you will use the default `concatenate` mode to join the documents coming from two Retrievers as the [Ranker](https://docs.haystack.deepset.ai/docs/rankers) will be the main component to rank the documents for relevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GYso6_8BrhY8"
   },
   "outputs": [],
   "source": [
    "from haystack.components.joiners import DocumentJoiner\n",
    "\n",
    "document_joiner = DocumentJoiner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8_jHzmosbC_"
   },
   "source": [
    "### 3) Rank the Results\n",
    "\n",
    "Use the [TransformersSimilarityRanker](https://docs.haystack.deepset.ai/docs/transformerssimilarityranker) that scores the relevancy of all retrieved documents for the given search query by using a cross encoder model. In this example, you will use [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) model to rank the retrieved documents but you can replace this model with other cross-encoder models on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cN0woIxHs4Ng"
   },
   "outputs": [],
   "source": [
    "from haystack.components.rankers import TransformersSimilarityRanker\n",
    "\n",
    "ranker = TransformersSimilarityRanker(model=\"BAAI/bge-reranker-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5jzzvUIstQ4"
   },
   "source": [
    "### 4) Create the Hybrid Retrieval Pipeline\n",
    "\n",
    "Add all initialized components to your pipeline and connect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "y9sKO2Azjrsh"
   },
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "\n",
    "hybrid_retrieval = Pipeline()\n",
    "hybrid_retrieval.add_component(\"text_embedder\", text_embedder)\n",
    "hybrid_retrieval.add_component(\"embedding_retriever\", embedding_retriever)\n",
    "hybrid_retrieval.add_component(\"bm25_retriever\", bm25_retriever)\n",
    "hybrid_retrieval.add_component(\"document_joiner\", document_joiner)\n",
    "hybrid_retrieval.add_component(\"ranker\", ranker)\n",
    "\n",
    "hybrid_retrieval.connect(\"text_embedder\", \"embedding_retriever\")\n",
    "hybrid_retrieval.connect(\"bm25_retriever\", \"document_joiner\")\n",
    "hybrid_retrieval.connect(\"embedding_retriever\", \"document_joiner\")\n",
    "hybrid_retrieval.connect(\"document_joiner\", \"ranker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii9x0gr9lXRT"
   },
   "source": [
    "### 5) Visualize the Pipeline (Optional)\n",
    "\n",
    "To understand how you formed a hybrid retrieval pipeline, use [draw()](https://docs.haystack.deepset.ai/docs/drawing-pipeline-graphs) method of the pipeline. If you're running this notebook on Google Colab, the generate file will be saved in \"Files\" section on the sidebar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rXHbHru0lXRY"
   },
   "outputs": [],
   "source": [
    "save_img_name = \"hybrid-retrieval.png\"\n",
    "save_image_path = os.path.join(notebook_dir, \"../data/hybrid_retrieval/\", save_img_name)\n",
    "hybrid_retrieval.draw(path=save_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIYV19l16PKC"
   },
   "source": [
    "## Testing the Hybrid Retrieval\n",
    "\n",
    "Pass the query to `text_embedder`, `bm25_retriever` and `ranker` and run the retrieval pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_properties = DataProcessing.load_prediction_properties()\n",
    "prediction_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "cf62bf3b3c6144629811874114dc527f",
      "0387b8e4546247f49f854f9729e6a3df",
      "d45dcf1e27a5401ca2c430cd6c322fdb",
      "864e853846b3406a9a8b743dbc1d96ff",
      "15f3697b990f40adb795dbcce3f626c1",
      "cf972cbfa1314f149b9860e199391170",
      "06c4d38f14674fd898aa85df17c0baa7",
      "a4d87d3d4a9a4dc4bccdc4a16be29096",
      "2398002062874506a558e59114359c54",
      "947922d2f75346cdbbb908d3888f832e",
      "c7a118b1f8a2491cacd5f76db668443e",
      "b395830b097f4862a6c8588ccd0fd91b",
      "4b9ea215e11844239e968960cd45cade",
      "097a7751c1b04aa3bf4a0a586a6dfa32",
      "9e5e169b22dd45068a2b03154b26ccf0",
      "827efd14f6f74c8ca0fb8df3a4359062",
      "2a7323e821724b129d01de39adb67c4f",
      "eeb568b15a1d465bab9f7a3843d611df",
      "c1626ca2ebf84556a692e76f16f3cab4",
      "5cd3d66281d247749d4c71daa7c6f49d",
      "107daf1c8f8f4203aff8058191ce9728",
      "31acc8ba63e240a6b4419ab4aa51c87a"
     ]
    },
    "id": "glS0-Xh3nLHY",
    "outputId": "eae6ba6e-136e-48a1-bfe3-3a97fcebca7c"
   },
   "outputs": [],
   "source": [
    "future_verbs = [\n",
    "    \"will\",\n",
    "    \"shall\",\n",
    "    \"would\",\n",
    "    \"going\",\n",
    "    \"might\",\n",
    "    \"should\",\n",
    "    \"could\",\n",
    "    \"may\",\n",
    "    \"must\",\n",
    "    \"can\"\n",
    "]\n",
    "query = f\"\"\"Can you identify the predictions from the documents? I define a prediction as: {prediction_properties}. Note that the documents should be future tense like {future_verbs}.\n",
    "    \n",
    "    Some examples of predictions in the PhraseBank dataset are \\n\n",
    "        1. According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing . \\n\n",
    "        2. According to the company 's updated strategy for the years 2009-2012 , Basware targets a long-term net sales growth in the range of 20 % -40 % with an operating profit margin of 10 % -20 % of net sales .\n",
    "        3. Its board of directors will propose a dividend of EUR0 .12 per share for 2010 , up from the EUR0 .08 per share paid in 2009 .\n",
    "    Some examples of non-predictions in the P\n",
    "        1. Net sales increased to EUR193 .3 m from EUR179 .9 m and pretax profit rose by 34.2 % to EUR43 .1 m. ( EUR1 = USD1 .4 )\n",
    "        2. Net sales surged by 18.5 % to EUR167 .8 m. Teleste said that EUR20 .4 m , or 12.2 % , of the sales came from the acquisitions made in 2009 .\n",
    "        3. STORA ENSO , NORSKE SKOG , M-REAL , UPM-KYMMENE Credit Suisse First Boston ( CFSB ) raised the fair value for shares in four of the largest Nordic forestry groups .\n",
    "\"\"\"\n",
    "\n",
    "retrieved_result = hybrid_retrieval.run(\n",
    "    {\"text_embedder\": {\"text\": query}, \"bm25_retriever\": {\"query\": query}, \"ranker\": {\"query\": query}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_result[\"ranker\"][\"documents\"]\n",
    "retrieved_result_df = pd.DataFrame(retrieved_result[\"ranker\"][\"documents\"])\n",
    "retrieved_result_df['query'] = query\n",
    "retrieved_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"If save as new file:\"\"\"\n",
    "\n",
    "# path = os.path.join(notebook_dir, \"../data/rag/retrieved/\")\n",
    "# prefix = \"retrieved_results\"\n",
    "# save_file_type = \"csv\"\n",
    "# DataProcessing.save_to_file(retrieved_result_df, path, prefix, save_file_type)\\\n",
    "\n",
    "\n",
    "\"\"\"If append to previous file, then save:\"\"\"\n",
    "path = os.path.join(notebook_dir, \"../data/rag/retrieved/\", \"text_label_name_meta_data-v1.csv\")\n",
    "# prefix = \"text_label_metadata\"\n",
    "save_file_type = \"csv\"\n",
    "df = DataProcessing.load_from_file(path, save_file_type)\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = retrieved_result_df.content\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_df = retrieved_result_df.drop(['content'], axis=1)\n",
    "meta_data_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(content)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_datas = []\n",
    "for idx, row in meta_data_df.iterrows():\n",
    "    meta_datas.append(row)\n",
    "\n",
    "meta_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['meta_data'] = meta_datas\n",
    "new_df.rename(columns={'content': 'text'}, inplace=True)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = pd.concat([df, new_df])\n",
    "updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(notebook_dir, \"../data/rag/retrieved/\")\n",
    "prefix = \"text_label_name_meta_data\"\n",
    "save_file_type = \"csv\"\n",
    "DataProcessing.save_to_file(updated_df, path, prefix, save_file_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvPv1cJ6gbBJ"
   },
   "source": [
    "### Pretty Print the Results\n",
    "Create a function to print a kind of *search page*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raL_z_sByDoQ"
   },
   "outputs": [],
   "source": [
    "def pretty_print_results(prediction):\n",
    "    for doc in prediction[\"documents\"]:\n",
    "        print(doc.content, \"\\t\", doc.score)\n",
    "        # print(doc.meta[\"abstract\"])\n",
    "        print(\"\\n\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSUiizGNytwX",
    "outputId": "38e05986-1737-4c2b-bc08-7399299de37f"
   },
   "outputs": [],
   "source": [
    "pretty_print_results(result[\"ranker\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ5_NJDz52VE"
   },
   "source": [
    "## What's next\n",
    "\n",
    "ðŸŽ‰ Congratulations! You've create a hybrid retrieval pipeline!\n",
    "\n",
    "If you'd like to use this retrieval method in a RAG pipeline, check out [Tutorial: Creating Your First QA Pipeline with Retrieval-Augmentation](https://haystack.deepset.ai/tutorials/27_first_rag_pipeline) to learn about the next steps.\n",
    "\n",
    "To stay up to date on the latest Haystack developments, you can [sign up for our newsletter](https://landing.deepset.ai/haystack-community-updates) or [join Haystack discord community](https://discord.gg/haystack).\n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
