{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08937f66",
   "metadata": {},
   "source": [
    "\n",
    "# Federal Open Market Committee (FOMC) Document Scraping Pipeline\n",
    "\n",
    "This notebook scrapes PDF documents from the Federal Reserve website, downloads them, and extracts their text content for analysis.\n",
    "\n",
    "The pipeline is broken down into the following steps:\n",
    "1.  **Configuration**: Set all parameters for the scrape.\n",
    "2.  **Core Functions**: Define the helper functions for the pipeline.\n",
    "3.  **Step 1: Create Directories**: Set up the folder structure.\n",
    "4.  **Step 2: Find PDF Links**: Scrape the website to find all target PDF URLs.\n",
    "5.  **Step 3: Download PDFs**: Download the discovered PDFs to a local directory.\n",
    "6.  **Step 4: Extract Text**: Process the downloaded PDFs to extract and save the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445337e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import pdfplumber\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "sys.path.append(os.path.join(notebook_dir, '../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ce3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_YEAR_CONFIG = 2018\n",
    "END_YEAR_CONFIG = 2019\n",
    "URL_TEMPLATE_CONFIG = \"https://www.federalreserve.gov/monetarypolicy/fomchistorical{year}.htm\"\n",
    "BASE_URL_CONFIG = \"https://www.federalreserve.gov\"\n",
    "KEYWORD_PATTERN_CONFIG = r'fomc\\d{8}tealbook[ab]\\d{8}'\n",
    "DOWNLOAD_LIMIT_CONFIG = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67681e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(pdf_dir: str, text_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates the necessary output directories if they don't already exist.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf_dir : str\n",
    "        The file path for the directory where PDF files will be saved.\n",
    "    text_dir : str\n",
    "        The file path for the directory where extracted text files will be saved.\n",
    "    \"\"\"\n",
    "    os.makedirs(pdf_dir, exist_ok=True)\n",
    "    os.makedirs(text_dir, exist_ok=True)\n",
    "    print(f\"Ensured directories exist: '{pdf_dir}/' and '{text_dir}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec19412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Creating Directories ---\n",
      "Ensured directories exist: '/Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/notebook_experiments/../data/federal_open_market_committee_dataset/tealbooks/pdfs/' and '/Users/detraviousjamaribrinkley/Documents/Development/research_labs/uf_ds/predictions/notebook_experiments/../data/federal_open_market_committee_dataset/tealbooks/text/'\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Step 1: Creating Directories ---\")\n",
    "BASE_DATA_PATH = os.path.join(notebook_dir, '../data')\n",
    "TEALBOOKS = \"tealbooks\"\n",
    "PDF_PATH_CONFIG = os.path.join(BASE_DATA_PATH, 'federal_open_market_committee_dataset', TEALBOOKS, 'pdfs')\n",
    "TEXT_PATH_CONFIG = os.path.join(BASE_DATA_PATH, 'federal_open_market_committee_dataset', TEALBOOKS, 'text')\n",
    "create_directories(pdf_dir=PDF_PATH_CONFIG, text_dir=TEXT_PATH_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c79e79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Generating Yearly URLs ---\n",
      "Generated 2 yearly pages to scrape from 2018 to 2019.\n",
      "Pages to scrape: ['https://www.federalreserve.gov/monetarypolicy/fomchistorical2018.htm', 'https://www.federalreserve.gov/monetarypolicy/fomchistorical2019.htm']\n"
     ]
    }
   ],
   "source": [
    "def generate_yearly_urls(start_year: int, end_year: int, url_template: str) -> list:\n",
    "    \"\"\"\n",
    "    Generates a list of yearly historical FOMC URLs based on a template.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_year : int\n",
    "        The first year in the desired range (inclusive).\n",
    "    end_year : int\n",
    "        The last year in the desired range (inclusive).\n",
    "    url_template : str\n",
    "        A string template for the URL, containing '{year}' as a placeholder.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of URLs, one for each year in the specified range.\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        urls.append(url_template.format(year=year))\n",
    "    print(f\"Generated {len(urls)} yearly pages to scrape from {start_year} to {end_year}.\")\n",
    "    return urls\n",
    "\n",
    "# --- Execute this step ---\n",
    "print(\"\\n--- Step 2: Generating Yearly URLs ---\")\n",
    "yearly_pages_to_scrape = generate_yearly_urls(\n",
    "    start_year=START_YEAR_CONFIG,\n",
    "    end_year=END_YEAR_CONFIG,\n",
    "    url_template=URL_TEMPLATE_CONFIG\n",
    ")\n",
    "print(\"Pages to scrape:\", yearly_pages_to_scrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7beeed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pdf_links(start_url: str, base_url: str, doc_keyword_pattern: str) -> set:\n",
    "    \"\"\"\n",
    "    Finds all unique PDF links on a single page that match a regex pattern.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_url : str\n",
    "        The URL of the page to scrape for links.\n",
    "    base_url : str\n",
    "        The base URL used to resolve relative links.\n",
    "    doc_keyword_pattern : str\n",
    "        A regex pattern used to identify the target links.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    set\n",
    "        A set of unique, absolute URLs found on the page.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching page: {start_url}\")\n",
    "    try:\n",
    "        response = requests.get(start_url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return set()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_links = set()\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        href = a_tag['href']\n",
    "        if href.endswith('.pdf') and re.search(doc_keyword_pattern, href, re.IGNORECASE):\n",
    "            full_url = urljoin(base_url, href)\n",
    "            pdf_links.add(full_url)\n",
    "    \n",
    "    print(f\"-> Found {len(pdf_links)} links matching pattern on this page.\")\n",
    "    return pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d4c755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Finding PDF Links ---\n",
      "Fetching page: https://www.federalreserve.gov/monetarypolicy/fomchistorical2018.htm\n",
      "-> Found 16 links matching pattern on this page.\n",
      "Fetching page: https://www.federalreserve.gov/monetarypolicy/fomchistorical2019.htm\n",
      "-> Found 16 links matching pattern on this page.\n",
      "\n",
      "Total unique links found across all years: 32\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 3: Finding PDF Links ---\")\n",
    "all_pdf_links = set()\n",
    "for page_url in yearly_pages_to_scrape:\n",
    "    links_from_page = find_pdf_links(\n",
    "        start_url=page_url,\n",
    "        base_url=BASE_URL_CONFIG,\n",
    "        doc_keyword_pattern=KEYWORD_PATTERN_CONFIG\n",
    "    )\n",
    "    all_pdf_links.update(links_from_page)\n",
    "print(f\"\\nTotal unique links found across all years: {len(all_pdf_links)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c50192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Downloading PDFs ---\n",
      "Limiting download to the newest 3 files.\n",
      "Downloading: FOMC20191211tealbookb20191205.pdf...\n",
      "Downloading: FOMC20191211tealbooka20191126.pdf...\n",
      "Downloading: FOMC20191030tealbookb20191024.pdf...\n"
     ]
    }
   ],
   "source": [
    "def download_pdfs(pdf_links: set, pdf_dir: str, num_to_download: int | str = \"all\") -> list:\n",
    "    \"\"\"\n",
    "    Downloads a set of PDFs from a list of URLs to a specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf_links : set\n",
    "        A set of absolute URLs for the PDF files to be downloaded.\n",
    "    pdf_dir : str\n",
    "        The local directory where the downloaded PDFs will be saved.\n",
    "    num_to_download : int or str, optional\n",
    "        The number of PDFs to download.\n",
    "    \"\"\"\n",
    "    local_pdf_paths = []\n",
    "    sorted_links = sorted(list(pdf_links), reverse=True)\n",
    "    \n",
    "    links_to_process = []\n",
    "    if isinstance(num_to_download, int):\n",
    "        print(f\"Limiting download to the newest {num_to_download} files.\")\n",
    "        links_to_process = sorted_links[:num_to_download]\n",
    "    else:\n",
    "        print(\"Preparing to download all found files.\")\n",
    "        links_to_process = sorted_links\n",
    "        \n",
    "    for url in links_to_process:\n",
    "        filename = url.split('/')[-1]\n",
    "        pdf_path = os.path.join(pdf_dir, filename)\n",
    "        \n",
    "        if os.path.exists(pdf_path):\n",
    "            print(f\"Skipping (already exists): {filename}\")\n",
    "        else:\n",
    "            print(f\"Downloading: {filename}...\")\n",
    "            try:\n",
    "                pdf_response = requests.get(url, timeout=30)\n",
    "                pdf_response.raise_for_status()\n",
    "                with open(pdf_path, 'wb') as f:\n",
    "                    f.write(pdf_response.content)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  -> Failed to download {url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        local_pdf_paths.append(pdf_path)\n",
    "    return local_pdf_paths\n",
    "\n",
    "# --- Execute this step ---\n",
    "print(\"\\n--- Step 4: Downloading PDFs ---\")\n",
    "local_pdf_paths = download_pdfs(\n",
    "    pdf_links=all_pdf_links,\n",
    "    pdf_dir=PDF_PATH_CONFIG,\n",
    "    num_to_download=DOWNLOAD_LIMIT_CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7163e3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5: Extracting Text ---\n",
      "Extracting text from: FOMC20191211tealbookb20191205.pdf...\n",
      "Extracting text from: FOMC20191211tealbooka20191126.pdf...\n",
      "Extracting text from: FOMC20191030tealbookb20191024.pdf...\n",
      "\n",
      "--- Pipeline Complete ---\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdfs(local_pdf_paths: list, text_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Extracts text from a list of local PDF files and saves it to a directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    local_pdf_paths : list\n",
    "        A list of local file paths to the PDF files that will be processed.\n",
    "    text_dir : str\n",
    "        The local directory where the extracted text files will be saved.\n",
    "    \"\"\"\n",
    "    for pdf_path in local_pdf_paths:\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        text_filename = filename.replace('.pdf', '.txt')\n",
    "        text_path = os.path.join(text_dir, text_filename)\n",
    "        \n",
    "        if os.path.exists(text_path):\n",
    "            print(f\"Skipping (text already extracted): {filename}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Extracting text from: {filename}...\")\n",
    "        full_text = \"\"\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for i, page in enumerate(pdf.pages):\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        full_text += page_text + f\"\\n\\n--- End of Page {i+1} ---\\n\\n\"\n",
    "            \n",
    "            with open(text_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(full_text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  -> Could not process {pdf_path}: {e}\")\n",
    "\n",
    "# --- Execute this step ---\n",
    "print(\"\\n--- Step 5: Extracting Text ---\")\n",
    "extract_text_from_pdfs(local_pdf_paths=local_pdf_paths, text_dir=TEXT_PATH_CONFIG)\n",
    "print(\"\\n--- Pipeline Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
